{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embeddings\n",
    "\n",
    "This week, we build on last week's topic modeling techniques by taking a text corpus we have developed, specifying an underlying number of dimensions, and training a model with a neural network auto-encoder (one of Google's word2vec  algorithms) that best describes corpus words in their local linguistic contexts, and exploring their locations in the resulting space to learn about the discursive culture that produced them. Documents here are represented as densely indexed locations in dimensions, rather than sparse mixtures of topics (as in LDA topic modeling), so that distances between those documents (and words) are consistently superior, though they require the full vector of dimension loadings (rather than just a few selected topic loadings) to describe. We will explore these spaces to understand complex, semantic relationships between words, index documents with descriptive words, identify the likelihood that a given document would have been produced by a given vector model, and explore how semantic categories can help us understand the cultures that produced them.\n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/czd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import gensim#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import nltk #For stop words and stemmers\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "\n",
    "#gensim uses a couple of deprecated features\n",
    "#we can't do anything about them so lets ignore them \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our corpora\n",
    "\n",
    "Instead of downloading our corpora, we have download them in advance; a subset of the [senate press releases](https://github.com/lintool/GrimmerSenatePressReleases) are in `grimmerPressReleases`. We will load them into a DataFrame, using a function from a couple weeks ago `loadTextDirectory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01Apr2005Kennedy14.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2005Kennedy12.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy10.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy11.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy12.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text\n",
       "01Apr2005Kennedy14.txt           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...\n",
       "01Aug2005Kennedy12.txt           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...\n",
       "01Aug2006Kennedy10.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...\n",
       "01Aug2006Kennedy11.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...\n",
       "01Aug2006Kennedy12.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kennedyDF = lucem_illud.loadTextDirectory('../data/grimmerPressReleases/Kennedy/')\n",
    "kennedyDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us one Senator's data, but with no metadata, we can add a category column with a simple command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01Apr2005Kennedy14.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2005Kennedy12.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy10.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy11.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy12.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text  \\\n",
       "01Apr2005Kennedy14.txt           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...   \n",
       "01Aug2005Kennedy12.txt           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...   \n",
       "01Aug2006Kennedy10.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...   \n",
       "01Aug2006Kennedy11.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...   \n",
       "01Aug2006Kennedy12.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...   \n",
       "\n",
       "                       category  \n",
       "01Apr2005Kennedy14.txt  Kennedy  \n",
       "01Aug2005Kennedy12.txt  Kennedy  \n",
       "01Aug2006Kennedy10.txt  Kennedy  \n",
       "01Aug2006Kennedy11.txt  Kennedy  \n",
       "01Aug2006Kennedy12.txt  Kennedy  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kennedyDF['category'] = 'Kennedy'\n",
    "kennedyDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also will be wanting to load all the senators so we will need to loop over all the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01Apr2005Kennedy14.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2005Kennedy12.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy10.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy11.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy12.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text  \\\n",
       "01Apr2005Kennedy14.txt           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...   \n",
       "01Aug2005Kennedy12.txt           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...   \n",
       "01Aug2006Kennedy10.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...   \n",
       "01Aug2006Kennedy11.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...   \n",
       "01Aug2006Kennedy12.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...   \n",
       "\n",
       "                       category  \n",
       "01Apr2005Kennedy14.txt  Kennedy  \n",
       "01Aug2005Kennedy12.txt  Kennedy  \n",
       "01Aug2006Kennedy10.txt  Kennedy  \n",
       "01Aug2006Kennedy11.txt  Kennedy  \n",
       "01Aug2006Kennedy12.txt  Kennedy  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = '../data/grimmerPressReleases'\n",
    "\n",
    "senReleasesDF = pandas.DataFrame()\n",
    "\n",
    "for senatorDir in (file for file in os.scandir(dataDir) if not file.name.startswith('.') and file.is_dir()):\n",
    "    senDF = lucem_illud.loadTextDirectory(senatorDir.path)\n",
    "    senDF['category'] = senatorDir.name\n",
    "    senReleasesDF = senReleasesDF.append(senDF, ignore_index = False)\n",
    "\n",
    "senReleasesDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to remove stop words and stem. Tokenizing requires two steps. Word2Vec needs to retain the sentence structure so as to capture a \"continuous bag of words (CBOW)\" and all of the skip-grams within a word window. The algorithm tries to preserve the distances induced by one of these two local structures. This is very different from clustering and LDA topic modeling which extract unordered words alone. As such, tokenizing is slightly more involved, but we can still use `lucem_illud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01Apr2005Kennedy14.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...</td>\n",
       "      <td>[[immediate, release, immediate, release, cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2005Kennedy12.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...</td>\n",
       "      <td>[[immediate, release, immediate, release, cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy10.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...</td>\n",
       "      <td>[[immediate, release, immediate, release, wash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy11.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...</td>\n",
       "      <td>[[immediate, release, immediate, release, wash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01Aug2006Kennedy12.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...</td>\n",
       "      <td>[[immediate, release, immediate, release, cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text  \\\n",
       "01Apr2005Kennedy14.txt           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...   \n",
       "01Aug2005Kennedy12.txt           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...   \n",
       "01Aug2006Kennedy10.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...   \n",
       "01Aug2006Kennedy11.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...   \n",
       "01Aug2006Kennedy12.txt           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...   \n",
       "\n",
       "                       category  \\\n",
       "01Apr2005Kennedy14.txt  Kennedy   \n",
       "01Aug2005Kennedy12.txt  Kennedy   \n",
       "01Aug2006Kennedy10.txt  Kennedy   \n",
       "01Aug2006Kennedy11.txt  Kennedy   \n",
       "01Aug2006Kennedy12.txt  Kennedy   \n",
       "\n",
       "                                                          tokenized_sents  \\\n",
       "01Apr2005Kennedy14.txt  [[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...   \n",
       "01Aug2005Kennedy12.txt  [[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...   \n",
       "01Aug2006Kennedy10.txt  [[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...   \n",
       "01Aug2006Kennedy11.txt  [[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...   \n",
       "01Aug2006Kennedy12.txt  [[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...   \n",
       "\n",
       "                                                         normalized_sents  \n",
       "01Apr2005Kennedy14.txt  [[immediate, release, immediate, release, cont...  \n",
       "01Aug2005Kennedy12.txt  [[immediate, release, immediate, release, cont...  \n",
       "01Aug2006Kennedy10.txt  [[immediate, release, immediate, release, wash...  \n",
       "01Aug2006Kennedy11.txt  [[immediate, release, immediate, release, wash...  \n",
       "01Aug2006Kennedy12.txt  [[immediate, release, immediate, release, cont...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply our functions, notice each row is a list of lists now\n",
    "senReleasesDF['tokenized_sents'] = senReleasesDF['text'].apply(lambda x: \n",
    "                                                               [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "senReleasesDF['normalized_sents'] = senReleasesDF['tokenized_sents'].apply(lambda x: \n",
    "                                                                           [lucem_illud.normalizeTokens(s, \n",
    "                                                                                           stopwordLst = lucem_illud.stop_words_basic, \n",
    "                                                                                           stemmer = None) \n",
    "                                                                            for s in x])\n",
    "\n",
    "senReleasesDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "We will be using the gensim implementation of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec).\n",
    "\n",
    "To load our data, we give all the sentences to the trainer. We just need to add the words as a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesW2V = gensim.models.word2vec.Word2Vec(senReleasesDF['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the word2vec object, the words each have a vector. To access the vector directly, use the square braces (`__getitem__`) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 100 dimesional vector:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.30339050e+00, -1.76357043e+00,  2.55048513e+00,  1.99355614e+00,\n",
       "       -1.35247302e+00,  9.70095277e-01, -1.73895434e-01, -7.47145787e-02,\n",
       "        1.02505839e+00, -2.21239591e+00, -1.85035735e-01, -1.66594759e-01,\n",
       "        2.43685341e+00,  1.83294261e+00, -5.38275599e-01,  1.53108999e-01,\n",
       "       -2.36452413e+00, -6.31232440e-01, -9.17102695e-01,  2.70953745e-01,\n",
       "        6.03676438e-01,  4.36816812e-01, -2.87314272e+00, -1.41489112e+00,\n",
       "       -1.42786431e+00, -7.24204481e-01,  1.60836220e+00, -1.02538121e+00,\n",
       "       -6.93277180e-01, -7.69155622e-01,  1.05041039e+00, -2.85863423e+00,\n",
       "       -1.43470085e+00, -7.12633789e-01,  3.49982947e-01,  8.08036253e-02,\n",
       "        1.93273529e-01,  3.72858071e+00, -1.64750361e+00,  1.76591003e+00,\n",
       "        1.87317097e+00,  5.24482787e-01,  1.17867041e+00,  8.79471302e-01,\n",
       "       -1.45901597e+00,  1.81700265e+00,  9.12630498e-01,  1.78307188e+00,\n",
       "       -2.12498045e+00, -1.19465148e+00,  1.73755622e+00, -1.94342184e+00,\n",
       "       -6.28403485e-01,  1.70434308e+00, -8.19669440e-02,  2.53659701e+00,\n",
       "       -8.96728635e-01,  6.88747525e-01,  2.18999520e-01, -8.49461436e-01,\n",
       "       -1.23273591e-02,  1.84314921e-02, -5.65451205e-01,  6.67195439e-01,\n",
       "       -7.91214883e-01,  1.18350363e+00, -3.86355817e-01,  9.55328867e-02,\n",
       "        4.59976226e-01, -8.95974219e-01,  3.39336041e-03, -5.02647817e-01,\n",
       "        4.41110551e-01, -3.33928317e-01,  2.71104550e+00, -3.32443379e-02,\n",
       "       -5.33736795e-02,  1.76706016e+00,  3.15144777e+00, -2.17744589e+00,\n",
       "       -4.00078386e-01,  1.98471236e+00, -8.85315895e-01, -5.55309296e-01,\n",
       "       -1.84656352e-01, -9.60160911e-01,  5.89570343e-01,  9.58586752e-01,\n",
       "        1.12997435e-01,  6.12558842e-01,  5.81041932e-01,  1.94141358e-01,\n",
       "        2.12304163e+00,  1.20417535e+00,  1.80065298e+00, -2.25034404e+00,\n",
       "       -5.77120721e-01, -7.12810516e-01,  3.08451319e+00,  3.47656071e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"A {} dimesional vector:\".format(senReleasesW2V['president'].shape[0]))\n",
    "senReleasesW2V['president']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want the full matrix, `syn0` stores all the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.62124555e-02,  5.37954867e-01, -1.13834751e+00, ...,\n",
       "        -4.50650871e-01,  1.20843887e+00, -1.37342036e+00],\n",
       "       [ 2.45421663e-01,  2.10670877e+00,  9.79076743e-01, ...,\n",
       "         6.49043083e-01,  1.04291117e+00, -4.61428203e-02],\n",
       "       [ 2.70196700e+00, -1.50793195e+00, -4.54737782e-01, ...,\n",
       "         2.69710533e-02,  1.15770131e-01, -1.25612617e+00],\n",
       "       ...,\n",
       "       [-2.46335510e-02,  4.66836914e-02,  1.02517210e-01, ...,\n",
       "         6.34911135e-02, -1.09182678e-01,  7.26778433e-02],\n",
       "       [ 2.16303170e-02,  2.38120439e-03,  8.79529491e-02, ...,\n",
       "         3.95261198e-02,  2.47571282e-02,  4.80741635e-03],\n",
       "       [ 7.62739927e-02,  4.76447269e-02,  1.01364315e-01, ...,\n",
       "         8.90460163e-02, -7.44280219e-02,  9.72193554e-02]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, `index2word` lets you translate from the matrix to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'american'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.wv.index2word[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a few things that come from the word vectors. The first is to find similar vectors (cosine similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('administration', 0.8000612258911133),\n",
       " ('presidents', 0.7179593443870544),\n",
       " ('administrations', 0.6878732442855835),\n",
       " ('cheney', 0.5809541344642639),\n",
       " ('george', 0.5648810267448425),\n",
       " ('responds', 0.5525197982788086),\n",
       " ('veto', 0.49843159317970276),\n",
       " ('republican', 0.4956115186214447),\n",
       " ('invaded', 0.4947235584259033),\n",
       " ('gop', 0.4871729016304016)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.most_similar('president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wars', 0.6791698932647705),\n",
       " ('descending', 0.6670769453048706),\n",
       " ('afghanistan', 0.6511711478233337),\n",
       " ('chaos', 0.6391745209693909),\n",
       " ('disobedience', 0.6274658441543579),\n",
       " ('quagmire', 0.6253349781036377),\n",
       " ('sliding', 0.61304771900177),\n",
       " ('foment', 0.5919989943504333),\n",
       " ('militarily', 0.5885466933250427),\n",
       " ('sending', 0.5839246511459351)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.most_similar('war')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can get this directly (calculated slightly differently):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_difference(embedding,word1,word2):\n",
    "    return sklearn.metrics.pairwise.cosine_similarity(embedding[word1].reshape(1,-1),embedding[word2].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4045713]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_difference(senReleasesW2V, 'war', 'unwinnable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find which word least matches the others within a word set (cosine similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'washington'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.doesnt_match(['administration', 'administrations', 'presidents', 'president', 'washington'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which word best matches the result of a semantic *equation* (here, we seek the words whose vectors best fit the missing entry from the equation: **X + Y - Z = _**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bush', 0.6530749797821045),\n",
       " ('bushs', 0.643796443939209),\n",
       " ('veto', 0.6324011087417603),\n",
       " ('signing', 0.6274088621139526),\n",
       " ('signature', 0.62520432472229),\n",
       " ('proposing', 0.6160760521888733),\n",
       " ('signed', 0.6159865856170654),\n",
       " ('sign', 0.6102637648582458),\n",
       " ('vetoed', 0.6074589490890503),\n",
       " ('reverse', 0.5957595109939575)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.most_similar(positive=['clinton', 'republican'], negative = ['democrat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that **Clinton + Republican - Democrat = Bush**. In other words, in this dataset and period, **Clinton** was to **Democrat** as **Bush** was to **Republican**. Whoah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " But how do we argue that these are stable distances or associations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Credible or Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boostrapping approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose well-established bootstrapping and subsampling methods to nonparametrically demonstrate the stability and significance of word associations within our embedding model. These allow us to establish conservative confidence intervals to both (a) distances between words in a model and (b) projections of words onto an induced dimension (e.g., man-woman). If we assume that the texts (e.g., newspapers, books) underlying our word embedding model are observations drawn from an independent and identically distributed (i.i.d.) population of cultural observations, then bootstrapping allows us to estimate the variance of word distances and projections by measuring those properties through sampling the empirical distribution of texts with replacement (Efron and Tibshirani 1994; Efron 2003). Operationally, if we wanted to bootstrap a 90% confidence interval of a word-word distance or word-dimension projection, we would sample a corpus the same size as the original corpus, but with replacement, 20 times, estimate word embedding models on each sample. Then we take the 2nd order (2nd smallest) statistic $s_{(2)}$--either distance or projection--as our confidence interval’s lower bound, and 19th order statistic $s_{(19)}$ as its upper bound. The distance between $s_{(2)}$ and $s_{(19)}$ across 20 bootstrap samples span the 5th to the 95th percentiles of the statistic’s variance, bounding the 90th confidence interval. A 95% confidence interval would span $s_{(2)}$ and $s_{(39)}$ in word embedding distances or projections estimated on 40 bootstrap samples of a corpus, tracing the 2.5th to 97.5th percentiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3077817,\n",
       " 0.31900012,\n",
       " 0.38668165,\n",
       " 0.4111389,\n",
       " 0.41611618,\n",
       " 0.4226749,\n",
       " 0.43276682,\n",
       " 0.43497938,\n",
       " 0.4604326,\n",
       " 0.49920022,\n",
       " 0.5308198,\n",
       " 0.5403556,\n",
       " 0.55404073,\n",
       " 0.5602777,\n",
       " 0.5710476,\n",
       " 0.59749997,\n",
       " 0.60074365,\n",
       " 0.635484]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimatesB=[]\n",
    "for x in range(20):\n",
    "    senReleasesW2VB = gensim.models.word2vec.Word2Vec(senReleasesDF['normalized_sents'].sample(frac=1.0, replace=True).sum())\n",
    "    try:\n",
    "        estimatesB.append(cos_difference(senReleasesW2VB, 'war', 'unwinnable')[0,0])\n",
    "    except KeyError:\n",
    "        #Missing one of the words from the vocab\n",
    "        pass\n",
    "                                                      \n",
    "estimatesB.sort()         \n",
    "estimatesB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 90% confidence interval for the cosine distance between war and unwinnable is:\n",
      " 0.31900012 0.60074365\n"
     ]
    }
   ],
   "source": [
    "print(\"The 90% confidence interval for the cosine distance between war and unwinnable is:\\n\",estimatesB[1], estimatesB[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the corpus is large, then we may take a subsampling approach, which randomly partitions the corpus into non-overlapping samples, then estimates the word-embedding models on these subsets and calculates confidence intervals as a function of the empirical distribution of distance or projection statistics and number of texts in the subsample (Politis and Romano 1997). Subsampling requires the same i.i.d. assumption as the bootrap (Politis and Romano 1992; Politis and Romano 1994). For 90% confidence intervals, we randomly partition the corpus into 20 subcorpora, then calculate $B^k=\\sqrt{\\tau_k}\\left(s^k-\\bar{s}\\right)$ for each $k$th sample, where $k$ is the number of texts and $s^k$ is the embedding distance or projection for the $k$th sample, and $\\bar{s}$ is the average statistic for all samples. The 90% confidence interval spans the 5th to 95th percentile variances, inscribed by $\\bar{s}-\\frac{B_{(19)}^k}{\\sqrt{\\tau}}$ and $\\bar{s}-\\frac{B_{(2)}^k}{\\sqrt{\\tau}}$, where $\\tau$ the number of texts in the total corpus and $s$ is the average statistic across all subsamples. As with bootrapping, a 95% confidence interval would require 40 subsamples; a 99% confidence would require 200 (.5th to 99.5th percentiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesDF[sample_indices == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "sample_indices = np.random.randint(0,n_samples,(len(senReleasesDF),))\n",
    "\n",
    "s_k =np.array([])\n",
    "tau_k=np.array([])\n",
    "\n",
    "for i in range(n_samples):\n",
    "    sample_w2v = gensim.models.word2vec.Word2Vec(senReleasesDF[sample_indices == i]['normalized_sents'].sum())\n",
    "    try:\n",
    "        #Need to use words present in most samples\n",
    "        s_k = np.append(s_k, cos_difference(sample_w2v, 'war', 'responsibility')[0,0])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    else:\n",
    "        tau_k = np.append(tau_k, len(senReleasesDF[sample_indices == i]))\n",
    "\n",
    "print(s_k)\n",
    "print(tau_k)\n",
    "\n",
    "tau = tau_k.sum()\n",
    "s = s_k.mean()\n",
    "B_k = np.sqrt(tau_k) * s_k-s_k.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"The 90% confidence interval for the cosine distance between war and responsibility is:\\n\",s-B_k[-2]/np.sqrt(tau), s-B_k[1]/np.sqrt(tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the vectors for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesW2V.save(\"senpressreleasesWORD2Vec.mm\")\n",
    "#Load with senReleasesW2V = gensim.models.word2vec.Word2Vec.load('senpressreleasesWORD2Vec.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use dimension reduction to visulize the vectors. We will start by selecting a subset we want to plot. Let's look at the top words from the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numWords = 150\n",
    "targetWords = senReleasesW2V.wv.index2word[:numWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extract their vectors and create our own smaller matrix that preserved the distances from the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsSubMatrix = []\n",
    "for word in targetWords:\n",
    "    wordsSubMatrix.append(senReleasesW2V[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use PCA to reduce the dimesions (e.g., to 50), and [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to project them down to the two we will visualize. We note that this is nondeterministic process, and so you can repeat and achieve alternative projectsions/visualizations of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords = sklearn.manifold.TSNE(n_components = 2, early_exaggeration = 25).fit_transform(reducedPCA_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can plot the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords):\n",
    "    ax.annotate(word, \n",
    "                (tsneWords[:, 0][i],tsneWords[:, 1][i]), \n",
    "                size =  20 * (numWords - i) / numWords, \n",
    "                alpha = .8 * (numWords - i) / numWords + .2)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My visualization above puts ``iraq`` next to ``time`` and ``bill`` near ``help``. <img src='../data/examplewordcloud.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if a new senator come along with whose names starts with K? Could we analyse their releases too without rerunning the entire embedding? Lets try with Cardin in `../data/grimmerPressReleases_extra/Cardin`. First we need to load and proccess the releases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cardinDF = lucem_illud.loadTextDirectory('../data/grimmerPressReleases_extra/Cardin')\n",
    "cardinDF['category'] = 'Cardin'\n",
    "\n",
    "cardinDF['tokenized_sents'] = cardinDF['text'].apply(lambda x: \n",
    "                                                               [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "cardinDF['normalized_sents'] = cardinDF['tokenized_sents'].apply(lambda x: \n",
    "                                                                           [lucem_illud.normalizeTokens(s, \n",
    "                                                                                           stopwordLst = lucem_illud.stop_words_basic, \n",
    "                                                                                           stemmer = None) \n",
    "                                                                            for s in x])\n",
    "\n",
    "cardinDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge Cardin's releases with the rest. This can update all the weights in *w2v* model, so be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is using a newer API so we have to tell it a little bit more for it to work right\n",
    "senReleasesW2V.build_vocab(cardinDF['normalized_sents'].sum(), update=True)\n",
    "senReleasesW2V.train(cardinDF['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, \n",
    "                     epochs=senReleasesW2V.iter)\n",
    "senReleasesW2V.save(\"senpressreleasesWORD2Vec_new.mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model is updated by this, we need to load our old copy to do a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesW2V_old = gensim.models.word2vec.Word2Vec.load('senpressreleasesWORD2Vec.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"A {} dimesional vector:\".format(senReleasesW2V['president'].shape[0]))\n",
    "senReleasesW2V['president'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"A {} dimesional vector:\".format(senReleasesW2V_old['president'].shape[0]))\n",
    "senReleasesW2V_old['president'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(senReleasesW2V_old['president'] - senReleasesW2V['president'])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the vector for 'president' has changed a little bit and the word cloud should also be a bit different too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords = sklearn.manifold.TSNE(n_components = 2, early_exaggeration = 25).fit_transform(reducedPCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords):\n",
    "    ax.annotate(word, \n",
    "                (tsneWords[:, 0][i],tsneWords[:, 1][i]), \n",
    "                size =  20 * (numWords - i) / numWords, \n",
    "                alpha = .8 * (numWords - i) / numWords + .2)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to consider in training is how we know that our parameters for the model are correct. We can do this by looking at the training loss of the model. Let's tart by training a new model, but this time we will expose most of the options and train it one epoch at a time. Look [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) for more detail: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesW2V_loss = gensim.models.word2vec.Word2Vec(size = 100, #dimensions\n",
    "                                                      alpha=0.025,\n",
    "                                                      window=5,\n",
    "                                                      min_count=5,\n",
    "                                                      hs=0,  #hierarchical softmax toggle\n",
    "                                                      compute_loss = True,\n",
    "                                                     )\n",
    "senReleasesW2V_loss.build_vocab(senReleasesDF['normalized_sents'].sum())\n",
    "senReleasesW2V_loss.train(senReleasesDF['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, \n",
    "                     epochs=1, #This the running_training_loss is a total so we have to do 1 epoch at a time\n",
    "                    )\n",
    "#Using a list so we can capture every epoch\n",
    "losses = [senReleasesW2V_loss.running_training_loss]\n",
    "losses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the training loss and can optimize training to minimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    senReleasesW2V_loss.train(senReleasesDF['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, \n",
    "                     epochs=1,\n",
    "                             )\n",
    "    losses.append(senReleasesW2V_loss.running_training_loss)\n",
    "    print(\"Done epoch {}\".format(i + 2), end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the loss vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lossesDF = pandas.DataFrame({'loss' : losses, 'epoch' : range(len(losses))})\n",
    "lossesDF.plot(y = 'loss', x = 'epoch', logy=False, figsize=(15, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the loss is almost monotonic, but that the rate decreases as epoch increases. Since we are testing on our training data monotonicity is a common result and we must try to avoid over fitting. A simple way to do this is to stop training when there is significant change in the rate of decrease. In this run, that looks to be approximately 8 or 9. If we were to do another analysis, we might use an `iter=9` instead of the default 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more culturally interesting is how many dimensions are required to achieve an optimal embedding. The use of words in complex ways and contradictory contexts will require more dimensions to represent them with integrity. For example, if one word, $w_a$, is \"nearby\" $w_b$, but $w_b$ is not near the other words beside $w_a$, then a new dimension will be required for the two words to be uniquely together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses_dims=[]\n",
    "\n",
    "for d in [50,100,150,200,250,300,350,400,450,500, 550, 600, 650, 700, 750]:\n",
    "    senReleasesW2V_loss_dims = gensim.models.word2vec.Word2Vec(size = d, #dimensions\n",
    "                                                      alpha=0.025,\n",
    "                                                      window=5,\n",
    "                                                      min_count=5,\n",
    "                                                      hs=0,  #hierarchical softmax toggle\n",
    "                                                      compute_loss = True,\n",
    "                                                     )\n",
    "    senReleasesW2V_loss_dims.build_vocab(senReleasesDF['normalized_sents'].sum())\n",
    "    senReleasesW2V_loss_dims.train(senReleasesDF['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, \n",
    "                     epochs=7, #This the running_training_loss is a total so we have to do 1 epoch at a time\n",
    "                    )\n",
    "    senReleasesW2V_loss_dims.train(senReleasesDF['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, \n",
    "                     epochs=1, #This the running_training_loss is a total so we have to do 1 epoch at a time\n",
    "                    )\n",
    "    \n",
    "    losses_dims.append(senReleasesW2V_loss_dims.running_training_loss/(10+d*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses_dimsDF = pandas.DataFrame({'loss' : losses_dims, 'dimensions' : [50,100,150,200,250,300,350,400,450,500,550,600,650,700,750]})\n",
    "losses_dimsDF.plot(y = 'loss', x = 'dimensions', logy=False, figsize=(15, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a work in progress for Gensim, but its clear that most of the word distance variation is captured by 300 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/1992embeddings_hs_new3.sg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8e9de430079b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/1992embeddings_hs_new3.sg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1570\u001b[0m         \u001b[0;31m# update older models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[0;34m\"\"\"Load pickled object from `fname`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3u'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[0;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0mraw_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/1992embeddings_hs_new3.sg'"
     ]
    }
   ],
   "source": [
    "model=gensim.models.Word2Vec.load('../data/1992embeddings_hs_new3.sg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analogy\n",
    "\n",
    "King+man-Queen? A few examples based on a corpus of Chinese news. \n",
    "\n",
    "First, location analogy: **province -> capital**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mm = model.most_similar(positive=[u'长沙',u'陕西'], negative=[u'湖南']) # Changsha + Shaanxi - Hunan\n",
    "for m in mm:\n",
    "    print(m[0],m[1])\n",
    "    print(\"Xi'an\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mm = model.most_similar(positive=[u'广州',u'湖北'], negative=[u'广东']) # Guangzhou + Hubei - Guangdong\n",
    "for m in mm:\n",
    "    print(m[0],m[1])\n",
    "    print(\"Wuhan\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, location analogy: **country -> capital**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mm = model.most_similar(positive=[u'东京',u'美国'], negative=[u'日本']) # Tokyo + US - Japan\n",
    "for m in mm:\n",
    "    print(m[0],m[1])\n",
    "    print(\"(Washington DC)\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = u'社会主义'  #socialism\n",
    "ss = model.similar_by_word(word,topn=10)\n",
    "print(\"the most similar words to \" + word + \" is: \")\n",
    "for s in ss:\n",
    "    print(s[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = u'玉米'  # corn\n",
    "ss = model.similar_by_word(word,topn=10)\n",
    "print(\"the most similar words to \" + word + \" is: \")\n",
    "for s in ss:\n",
    "    print(s[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that build a word2vec model with your corpus. Interrogate word relationships in the resulting space, including estimating 90% confidence intervals for specific word cosine distances of interest. Plot a subset of your words. What do these word relationships reveal about the *social* and *cultural game* underlying your corpus? What was surprising--what violated your prior understanding of the corpus? What was expected--what confirmed your knowledge about this domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Corpora: Trump 100 Days Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @jmichaelkell: To sum it up:\\r\\r1. Trump - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @DrDavidDuke: Did John McCain wiretap Presi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @PrisonPlanet: The Revolutionary Communist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Team Trump shares tips on keeping their boss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @larryelder: Rep. @MaxineWaters says Trump'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  RT @jmichaelkell: To sum it up:\\r\\r1. Trump - ...\n",
       "1  RT @DrDavidDuke: Did John McCain wiretap Presi...\n",
       "2  RT @PrisonPlanet: The Revolutionary Communist ...\n",
       "3  \"Team Trump shares tips on keeping their boss ...\n",
       "4  RT @larryelder: Rep. @MaxineWaters says Trump'..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trumpDF = pandas.read_csv(\"../data/trump100days.csv\")\n",
    "\n",
    "trumpDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words and stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @jmichaelkell: To sum it up:\\r\\r1. Trump - ...</td>\n",
       "      <td>[[RT, @, jmichaelkell, :, To, sum, it, up, :, ...</td>\n",
       "      <td>[[rt, jmichaelkell, sum], [trump, sexual, pred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @DrDavidDuke: Did John McCain wiretap Presi...</td>\n",
       "      <td>[[RT, @, DrDavidDuke, :, Did, John, McCain, wi...</td>\n",
       "      <td>[[rt, drdavidduke, john, mccain, wiretap, pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @PrisonPlanet: The Revolutionary Communist ...</td>\n",
       "      <td>[[RT, @, PrisonPlanet, :, The, Revolutionary, ...</td>\n",
       "      <td>[[rt, prisonplanet, revolutionary, communist, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Team Trump shares tips on keeping their boss ...</td>\n",
       "      <td>[[``, Team, Trump, shares, tips, on, keeping, ...</td>\n",
       "      <td>[[team, trump, shares, tips, keeping, boss, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @larryelder: Rep. @MaxineWaters says Trump'...</td>\n",
       "      <td>[[RT, @, larryelder, :, Rep., @, MaxineWaters,...</td>\n",
       "      <td>[[rt, larryelder, maxinewaters, says, trump, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RT @jmichaelkell: To sum it up:\\r\\r1. Trump - ...   \n",
       "1  RT @DrDavidDuke: Did John McCain wiretap Presi...   \n",
       "2  RT @PrisonPlanet: The Revolutionary Communist ...   \n",
       "3  \"Team Trump shares tips on keeping their boss ...   \n",
       "4  RT @larryelder: Rep. @MaxineWaters says Trump'...   \n",
       "\n",
       "                                     tokenized_sents  \\\n",
       "0  [[RT, @, jmichaelkell, :, To, sum, it, up, :, ...   \n",
       "1  [[RT, @, DrDavidDuke, :, Did, John, McCain, wi...   \n",
       "2  [[RT, @, PrisonPlanet, :, The, Revolutionary, ...   \n",
       "3  [[``, Team, Trump, shares, tips, on, keeping, ...   \n",
       "4  [[RT, @, larryelder, :, Rep., @, MaxineWaters,...   \n",
       "\n",
       "                                    normalized_sents  \n",
       "0  [[rt, jmichaelkell, sum], [trump, sexual, pred...  \n",
       "1  [[rt, drdavidduke, john, mccain, wiretap, pres...  \n",
       "2  [[rt, prisonplanet, revolutionary, communist, ...  \n",
       "3  [[team, trump, shares, tips, keeping, boss, di...  \n",
       "4  [[rt, larryelder, maxinewaters, says, trump, c...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply our functions, notice each row is a list of lists now\n",
    "trumpDF['tokenized_sents'] = trumpDF['text'].apply(lambda x: \n",
    "                                                               [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "trumpDF['normalized_sents'] = trumpDF['tokenized_sents'].apply(lambda x: \n",
    "                                                                           [lucem_illud.normalizeTokens(s, \n",
    "                                                                                           stopwordLst = lucem_illud.stop_words_basic, \n",
    "                                                                                           stemmer = None) \n",
    "                                                                            for s in x])\n",
    "\n",
    "trumpDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trumpW2V = gensim.models.word2vec.Word2Vec(trumpDF['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"A {} dimesional vector:\".format(trumpW2V['america'].shape[0]))\n",
    "trumpW2V['america']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trumpW2V.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trumpW2V.wv.index2word[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trumpW2V.most_similar('america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trumpW2V.most_similar('mexico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_difference(embedding,word1,word2):\n",
    "    return sklearn.metrics.pairwise.cosine_similarity(embedding[word1].reshape(1,-1),embedding[word2].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_difference(trumpW2V,'america','mexico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trumpW2V.doesnt_match(['administration', 'administrations', 'presidents', 'president', 'washington'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Instead of just looking at just how words embed within in the space, we can look at how the different documents relate to each other within the space. First lets load our data--abstracts of most U.S. physics papers from the 1950s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsDF = pandas.read_csv('../data/APSabstracts1950s.csv', index_col = 0)\n",
    "apsDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load these as documents into Word2Vec, but first we need to normalize and pick some tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = ['photomagnetoelectric', 'quantum', 'boltzmann', 'proton', 'positron', 'feynman', 'classical', 'relativity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsDF['tokenized_words'] = apsDF['abstract'].apply(lambda x: nltk.word_tokenize(x))\n",
    "apsDF['normalized_words'] = apsDF['tokenized_words'].apply(lambda x: lucem_illud.normalizeTokens(x, stopwordLst = lucem_illud.stop_words_basic, stemmer = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taggedDocs = []\n",
    "for index, row in apsDF.iterrows():\n",
    "    #Just doing a simple keyword assignment\n",
    "    docKeywords = [s for s in keywords if s in row['normalized_words']]\n",
    "    docKeywords.append(row['copyrightYear'])\n",
    "    docKeywords.append(row['doi']) #This lets us extract individual documnets since doi's are unique\n",
    "    taggedDocs.append(gensim.models.doc2vec.LabeledSentence(words = row['normalized_words'], tags = docKeywords))\n",
    "apsDF['TaggedAbstracts'] = taggedDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a Doc2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V = gensim.models.doc2vec.Doc2Vec(apsDF['TaggedAbstracts'], size = 100) #Limiting to 100 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get vectors for the tags/documents, just as we did with words. Documents are actually the centroids (high dimensional average points) of their words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apsD2V.docvecs[1952]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words can still be accessed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V['atom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still use the ``most_similar`` command to perform simple semantic equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V.most_similar(positive = ['atom','electrons'], negative = ['electron'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. **Electron** is to **electrons** as **atom** is to **atoms**. Another way to understand this, developed below is: **electrons - electron** induces a singular to plural dimension, so when we subtract **electron** from **atom** and add **electrons**, we get **atoms**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V.most_similar(positive = ['einstein','law'], negative = ['equation'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words **Einstein** minus **equation** plus **law** equals **Meissner**--Walthur Meissner studied mechanical engineering and physics ... and was more likely to produce a \"law\" than a \"equation\", like the Meissner effect, the damping of the magnetic field in superconductors. If we built our word-embedding with a bigger corpus like the entire arXiv, a massive repository of physics preprints, we would see many more such relationships like **gravity - Newton + Einstein = relativity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute all of these *by hand*--explicitly wth vector algebra: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.pairwise.cosine_similarity(apsD2V['electron'].reshape(1,-1), apsD2V['positron'].reshape(1,-1))\n",
    "#We reorient the vectors with .reshape(1, -1) so that they can be computed without a warning in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the doc2vec model, the documents have vectors just as the words do, so that we can compare documents with each other and also with words (similar to how a search engine locates a webpage with a query). First, we will calculate the distance between a word and documents in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V.docvecs.most_similar([ apsD2V['electron'] ], topn=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we search for the first of these on the web (these are doi codes), we find the following...a pretty good match:\n",
    "<img src='../data/PhysRev.98.875.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go the other way around and find words most similar to this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V.most_similar( [ apsD2V.docvecs['10.1103/PhysRev.98.875'] ], topn=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even look for documents most like a query composed of multiple words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V.docvecs.most_similar([ apsD2V['electron']+apsD2V['positron']+apsD2V['neutron']], topn=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot some words and documents against one another with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heatmapMatrix = []\n",
    "for tagOuter in keywords:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in keywords:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrix.append(column)\n",
    "heatmapMatrix = np.array(heatmapMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrix, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrix.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrix.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(keywords, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of similarities between the first ten documents in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targetDocs = apsDF['doi'][:10]\n",
    "\n",
    "heatmapMatrixD = []\n",
    "\n",
    "for tagOuter in targetDocs:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in targetDocs:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrixD.append(column)\n",
    "heatmapMatrixD = np.array(heatmapMatrixD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixD, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixD.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixD.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(targetDocs, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of similarities between the first ten documents and our keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heatmapMatrixC = []\n",
    "\n",
    "for tagOuter in targetDocs:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in keywords:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrixC.append(column)\n",
    "heatmapMatrixC = np.array(heatmapMatrixC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixC, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixC.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixC.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the model in case we would like to use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V.save('apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can later load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apsD2V = gensim.models.word2vec.Word2Vec.load('data/apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that build a doc2vec model with your corpus. Interrogate document and word relationships in the resulting space. Construct a heatmap that plots the distances between a subset of your documents against each other, and against a set of informative words. Find distances between *every* document in your corpus and a word or query of interest. What do these doc-doc proximities reveal about your corpus? What do these word-doc proximities highlight? Demonstrate and document one reasonable way to select a defensible subset of query-relevant documents for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also project word vectors to an arbitray semantic dimension. To demonstrate this possibility, let's first load a model trained with New York Times news articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nytimes_model = gensim.models.KeyedVectors.load_word2vec_format('../data/nytimes_cbow.reduced.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can visualize with dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words to create dimensions\n",
    "tnytTargetWords = ['man','him','he', 'woman', 'her', 'she', 'black','blacks','African', 'white', 'whites', 'Caucasian', 'rich', 'richer', 'richest', 'expensive', 'wealthy', 'poor', 'poorer', 'poorest', 'cheap', 'inexpensive']\n",
    "#words we will be mapping\n",
    "tnytTargetWords += [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\", \"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\", \"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]\n",
    "\n",
    "\n",
    "wordsSubMatrix = []\n",
    "for word in tnytTargetWords:\n",
    "    wordsSubMatrix.append(nytimes_model[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcaWordsNYT = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_dataNYT = pcaWordsNYT.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWordsNYT = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_dataNYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWordsNYT[:, 0], tsneWordsNYT[:, 1], alpha = 0) #Making the points invisible\n",
    "for i, word in enumerate(tnytTargetWords):\n",
    "    ax.annotate(word, (tsneWordsNYT[:, 0][i],tsneWordsNYT[:, 1][i]), size =  20 * (len(tnytTargetWords) - i) / len(tnytTargetWords))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some convenient functions for getting dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    normalized_vector = vector / np.linalg.norm(vector)\n",
    "    return normalized_vector\n",
    "\n",
    "def dimension(model, positives, negatives):\n",
    "    diff = sum([normalize(model[x]) for x in positives]) - sum([normalize(model[y]) for y in negatives])\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate three dimensions: gender, race, and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Gender = dimension(nytimes_model, ['man','him','he'], ['woman', 'her', 'she'])\n",
    "Race = dimension(nytimes_model, ['black','blacks','African'], ['white', 'whites', 'Caucasian'])\n",
    "Class = dimension(nytimes_model, ['rich', 'richer', 'richest', 'expensive', 'wealthy'], ['poor', 'poorer', 'poorest', 'cheap', 'inexpensive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Occupations = [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\"]\n",
    "\n",
    "Foods = [\"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\"]\n",
    "\n",
    "Sports  = [\"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to project words in a word list to each of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeDF(model, word_list):\n",
    "    g = []\n",
    "    r = []\n",
    "    c = []\n",
    "    for word in word_list:\n",
    "        g.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Gender.reshape(1,-1))[0][0])\n",
    "        r.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Race.reshape(1,-1))[0][0])\n",
    "        c.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Class.reshape(1,-1))[0][0])\n",
    "    df = pandas.DataFrame({'gender': g, 'race': r, 'class': c}, index = word_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OCCdf = makeDF(nytimes_model, Occupations) \n",
    "Fooddf = makeDF(nytimes_model, Foods)\n",
    "Sportsdf = makeDF(nytimes_model, Sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some useful functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Coloring(Series):\n",
    "    x = Series.values\n",
    "    y = x-x.min()\n",
    "    z = y/y.max()\n",
    "    c = list(plt.cm.rainbow(z))\n",
    "    return c\n",
    "\n",
    "def PlotDimension(ax,df, dim):\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_title(dim, fontsize = 20)\n",
    "    colors = Coloring(df[dim])\n",
    "    for i, word in enumerate(df.index):\n",
    "        ax.annotate(word, (0, df[dim][i]), color = colors[i], alpha = 0.6, fontsize = 12)\n",
    "    MaxY = df[dim].max()\n",
    "    MinY = df[dim].min()\n",
    "    plt.ylim(MinY,MaxY)\n",
    "    plt.yticks(())\n",
    "    plt.xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the occupational words in each of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, OCCdf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, OCCdf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, OCCdf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, Fooddf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, Fooddf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, Fooddf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, Sportsdf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, Sportsdf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, Sportsdf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that identify semantic dimensions of interest from your data (e.g., gender: man-woman) and project words onto these dimensions. Plot the array of relevant words along each semantic dimension. Which words are most different. Which dimensions are most different? On which dimension are your words most different? Print three short textual examples from the corpus that illustrate the association you have explored.\n",
    "\n",
    "<span style=\"color:red\">***Stretch***: Project documents from your corpus along a dimension of interest. Sample relevant documents from your corpus with this functionality and explain your rationale? Calculate the cosine of the angle between two dimensions (encoded as vectors) of interest. What does this suggest about the relationship between them within your corpus? \n",
    "\n",
    "<span style=\"color:red\">***Stretch***: Create 90% bootstrap confidence intervals around your word projections onto a given dimension. Which words are *significantly* different on your semantic dimension of interest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
