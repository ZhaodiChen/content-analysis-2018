{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Information Extraction\n",
    "\n",
    "\n",
    "This week, we move from arbitrary textual classification to the use of computation and linguistic models to parse precise claims from documents. Rather than focusing on simply the *ideas* in a corpus, here we focus on understanding and extracting its precise *claims*. This process involves a sequential pipeline of classifying and structuring tokens from text, each of which generates potentially useful data for the content analyst. Steps in this process, which we examine in this notebook, include: 1) tagging words by their part of speech (POS) to reveal the linguistic role they play in the sentence (e.g., Verb, Noun, Adjective, etc.); 2) tagging words as named entities (NER) such as places or organizations; 3) structuring or \"parsing\" sentences into nested phrases that are local to, describe or depend on one another; and 4) extracting informational claims from those phrases, like the Subject-Verb-Object (SVO) triples we extract here. While much of this can be done directly in the python package NLTK that we introduced in week 2, here we use NLTK bindings to the Stanford NLP group's open software, written in Java. Try typing a sentence into the online version [here](http://nlp.stanford.edu:8080/corenlp/) to get a sense of its potential. It is superior in performance to NLTK's implementations, but takes time to run, and so for these exercises we will parse and extract information for a very small text corpus. Of course, for final projects that draw on these tools, we encourage you to install the software on your own machines or shared servers at the university (RCC, SSRC) in order to perform these operations on much more text. \n",
    "\n",
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#For NLP\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.parse import stanford\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.draw.tree import TreeView\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to run this _once_ to download everything, you will also need [Java 1.8+](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) if you are using Windows or MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting downloads, this will take 5-10 minutes\n",
      "../stanford-NLP/parser already exists, skipping download\n",
      "Downloading ner from https://nlp.stanford.edu/software/stanford-ner-2017-06-09.zip\n",
      "Downloaded ner, extracting to ../stanford-NLP/ner\n",
      "Downloading postagger from https://nlp.stanford.edu/software/stanford-postagger-full-2017-06-09.zip\n",
      "Downloaded postagger, extracting to ../stanford-NLP/postagger\n",
      "Downloading core from http://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip\n",
      "Downloaded core, extracting to ../stanford-NLP/core\n",
      "Done setting up the Stanford NLP collection\n"
     ]
    }
   ],
   "source": [
    "lucem_illud.setupStanfordNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/czd/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "/Users/czd/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import lucem_illud.stanford as stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server on http://localhost:9000 , please wait a few seconds\n",
      "click Kernel -> Then Interupt to stop         (((っ･д･)っ               \n",
      "Exiting (ノ≧▽≦)ノ\n"
     ]
    }
   ],
   "source": [
    "stanford.startCoreServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the elephant in my pajamas.\n",
      "The quick brown fox jumped over the lazy dog.\n",
      "While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.\n",
      "Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.\n",
      "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\n"
     ]
    }
   ],
   "source": [
    "text = ['I saw the elephant in my pajamas.', 'The quick brown fox jumped over the lazy dog.', 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.', 'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.', 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']\n",
    "tokenized_text = [word_tokenize(t) for t in text]\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this homework, I intend to extract claims from cigarettes advertising slogans and anti-smoking slogans, to see what are the different stories behind the slogans.\n",
    "\n",
    "My work contains 4 parts: POS Tagging, Name Entity Recognition, Parsing and Information Extraction.\n",
    "\n",
    "To this end, I will use two modest datasets:\n",
    "\n",
    "1) **Cigarettes Advertisng Slogans**: This corpus contains 61 slogans in total, which credit to SRITA (Stanford University Research into the Impact of Tobacco Advertising). These slogans are from some famous cigarettes brands including Marlboro, Camel, Chesterfield, Lucky Strikes, Embassy, Newport, parliament, Kent and Kools.\n",
    "\n",
    "2) **Anti-smoking Slogans**: This corpus contains 68 slogans in total, which I collected from Brandongaille (“Marketing Expert & Blog Master”), a website shares marketing materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform POS tagging on a meaningful (but modest) subset of a corpus associated with your final project. Examine the list of words associated with at least three different parts of speech. Consider conditional frequencies (e.g., adjectives associated with nouns of interest or adverbs with verbs of interest). What do these distributions suggest about your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I am going to perform POS tagging on my two subset respectively, to classify each word by its semantic role in a sentence. Specifically, I will look into nouns, verbs and adjectives, and also conditional frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advertisement Slogan\n",
    "\n",
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smokeDF = pandas.read_excel('../data/smoke.xls', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>slogan-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Camel</td>\n",
       "      <td>More doctors smoke Camels than any other cigar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Camel</td>\n",
       "      <td>Not one single case of throat irritation!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Camel</td>\n",
       "      <td>See how your throat reacts to the delightfully...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Camel</td>\n",
       "      <td>Your \" T-Zone\" will tell you! T for taste, T f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Camel</td>\n",
       "      <td>For digestions sake, smoke Camels</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand                                        slogan-text\n",
       "0  Camel  More doctors smoke Camels than any other cigar...\n",
       "1  Camel          Not one single case of throat irritation!\n",
       "2  Camel  See how your throat reacts to the delightfully...\n",
       "3  Camel  Your \" T-Zone\" will tell you! T for taste, T f...\n",
       "4  Camel                  For digestions sake, smoke Camels"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smokeDF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smokeDF['sentences'] = smokeDF['slogan-text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>slogan-text</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Camel</td>\n",
       "      <td>More doctors smoke Camels than any other cigar...</td>\n",
       "      <td>[[More, doctors, smoke, Camels, than, any, oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Camel</td>\n",
       "      <td>Not one single case of throat irritation!</td>\n",
       "      <td>[[Not, one, single, case, of, throat, irritati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Camel</td>\n",
       "      <td>See how your throat reacts to the delightfully...</td>\n",
       "      <td>[[See, how, your, throat, reacts, to, the, del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Camel</td>\n",
       "      <td>Your \" T-Zone\" will tell you! T for taste, T f...</td>\n",
       "      <td>[[Your, ``, T-Zone, '', will, tell, you, !], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Camel</td>\n",
       "      <td>For digestions sake, smoke Camels</td>\n",
       "      <td>[[For, digestions, sake, ,, smoke, Camels]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand                                        slogan-text  \\\n",
       "0  Camel  More doctors smoke Camels than any other cigar...   \n",
       "1  Camel          Not one single case of throat irritation!   \n",
       "2  Camel  See how your throat reacts to the delightfully...   \n",
       "3  Camel  Your \" T-Zone\" will tell you! T for taste, T f...   \n",
       "4  Camel                  For digestions sake, smoke Camels   \n",
       "\n",
       "                                           sentences  \n",
       "0  [[More, doctors, smoke, Camels, than, any, oth...  \n",
       "1  [[Not, one, single, case, of, throat, irritati...  \n",
       "2  [[See, how, your, throat, reacts, to, the, del...  \n",
       "3  [[Your, ``, T-Zone, '', will, tell, you, !], [...  \n",
       "4        [[For, digestions, sake, ,, smoke, Camels]]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smokeDF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smokeDF['POS_sents'] = smokeDF['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[(More, JJR), (doctors, NNS), (smoke, VBP), (...\n",
       "1     [[(Not, RB), (one, CD), (single, JJ), (case, N...\n",
       "2     [[(See, VB), (how, WRB), (your, PRP$), (throat...\n",
       "3     [[(Your, PRP$), (``, ``), (T-Zone, NN), ('', '...\n",
       "4     [[(For, IN), (digestions, NNS), (sake, NN), (,...\n",
       "5     [[(Give, VB), (your, PRP$), (throat, NN), (a, ...\n",
       "6     [[(They, PRP), (’, VBP), (re, RB), (smooth, JJ...\n",
       "7     [[(It, PRP), (takes, VBZ), (healthy, JJ), (ner...\n",
       "8     [[(Camels, NNS), (never, RB), (get, VB), (on, ...\n",
       "9     [[(They, PRP), (don, VBP), (’, CD), (t, NN), (...\n",
       "10    [[(Fatigued, VBN), (?, .)], [(Get, VB), (a, DT...\n",
       "11    [[(Camels, NNS), (agree, VBP), (with, IN), (yo...\n",
       "12    [[(It, PRP), (’, RB), (s, VBZ), (a, DT), (psyc...\n",
       "13    [[(Ivory, NNP), (tips, NNS), (protect, VBP), (...\n",
       "14    [[(Gee, NNP), (,, ,), (Mommy, NNP), (you, PRP)...\n",
       "15    [[(Refreshes, NNS), (as, IN), (you, PRP), (smo...\n",
       "16    [[(It, PRP), (’, RB), (s, VBZ), (toasted, VBN)...\n",
       "17    [[(No, DT), (Throat, FW), (Irritation—No, FW),...\n",
       "18    [[(Toasting, NNP), (removes, VBZ), (dangerous,...\n",
       "19    [[(The, DT), (finest, JJS), (flavor, NN), (and...\n",
       "20    [[(11,105, CD), (doctors, NNS), (say, VBP), (L...\n",
       "21    [[(20,679, CD), (Physicians, NNP), (say, VBP),...\n",
       "22    [[(Ask, VB), (your, PRP$), (doctor, NN), (abou...\n",
       "23    [[(7, CD), (out, IN), (of, IN), (10, CD), (inh...\n",
       "24    [[(Luckies, NNS), (are, VBP), (easy, JJ), (on,...\n",
       "25    [[(Luckies, NNS), (are, VBP), (always, RB), (k...\n",
       "26    [[(Reach, VB), (for, IN), (a, DT), (Lucky, NNP...\n",
       "27    [[(I, PRP), (light, VBP), (a, DT), (Lucky, NNP...\n",
       "28    [[(No, DT), (excess, JJ), (weight, NN), (,, ,)...\n",
       "29    [[(Don, NNP), (’, NNP), (t, NN), (rasp, NN), (...\n",
       "                            ...                        \n",
       "31    [[(I, PRP), (protect, VBP), (my, PRP$), (voice...\n",
       "32    [[(Sensitive, JJ), (throats, NNS), (welcome, J...\n",
       "33    [[(There, EX), (is, VBZ), (never, RB), (a, DT)...\n",
       "34    [[(Smoke, NN), (to, TO), (your, PRP$), (throat...\n",
       "35    [[(Gentle, JJ), (on, IN), (your, PRP$), (throa...\n",
       "36    [[(Chesterfield, NNP), (is, VBZ), (best, JJS),...\n",
       "37    [[(First, JJ), (to, TO), (present, JJ), (scien...\n",
       "38    [[(..., :), (they, PRP), (leave, VBP), (a, DT)...\n",
       "39    [[(Nose, NN), (,, ,), (throat, NN), (,, ,), (a...\n",
       "40    [[(The, DT), (largest, JJS), (selling, NN), (c...\n",
       "41    [[(No, DT), (other, JJ), (cigarette, NN), (app...\n",
       "42    [[(For, IN), (the, DT), (greatest, JJS), (prot...\n",
       "43    [[(Your, PRP$), (throat, NN), (will, MD), (lik...\n",
       "44    [[(Doctors, NNS), (..., :), (aree, VBP), (that...\n",
       "45    [[(Got, VBD), (a, DT), (cold, NN), (?, .)], [(...\n",
       "46    [[(Those, DT), (holiday, NN), (throats, NNS), ...\n",
       "47    [[(For, IN), (your, PRP$), (throats, NNS), (sa...\n",
       "48    [[(Just, RB), (what, WP), (the, DT), (doctor, ...\n",
       "49                 [[(Mild, JJ), (As, IN), (May, NNP)]]\n",
       "50    [[(Chesterfield, NNP), (., .)], [(Blow, VB), (...\n",
       "51    [[(Slow, VB), (down, RB), (., .)], [(Pleasure,...\n",
       "52    [[(I, PRP), ('d, MD), (walk, VB), (a, DT), (mi...\n",
       "53    [[(Come, VB), (to, TO), (Marlboro, NNP), (Coun...\n",
       "54    [[(Come, VB), (to, TO), (where, WRB), (the, DT...\n",
       "55    [[(You, PRP), (get, VBP), (a, DT), (lot, NN), ...\n",
       "56    [[(You, PRP), (’, FW), (re, FW), (so, RB), (sm...\n",
       "57    [[(Inhale, VB), (to, TO), (your, PRP$), (heart...\n",
       "58    [[(They, PRP), (are, VBP), (so, RB), (smooth, ...\n",
       "59    [[(They, PRP), (certainly, RB), (do, VBP), (no...\n",
       "60    [[(Made, VBN), (especially, RB), (to, TO), (pr...\n",
       "Name: POS_sents, Length: 61, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smokeDF['POS_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the nouns (NN, NNS, NNP, NNPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('throat', 19),\n",
       " ('cigarette', 4),\n",
       " ('irritation', 3),\n",
       " ('taste', 3),\n",
       " ('protection', 3),\n",
       " ('T', 2),\n",
       " ('smoke', 2),\n",
       " ('t', 2),\n",
       " ('Pleasure', 2),\n",
       " ('flavor', 2),\n",
       " ('doctor', 2),\n",
       " ('s', 2),\n",
       " ('content', 2),\n",
       " ('smoking', 2),\n",
       " ('case', 1),\n",
       " ('mildness', 1),\n",
       " ('T-Zone', 1),\n",
       " ('sake', 1),\n",
       " ('vacation', 1),\n",
       " ('wind', 1),\n",
       " ('lift', 1),\n",
       " ('camel', 1),\n",
       " ('fact', 1),\n",
       " ('disposition', 1),\n",
       " ('cough', 1),\n",
       " ('light', 1),\n",
       " ('weight', 1),\n",
       " ('answer', 1),\n",
       " ('rasp', 1),\n",
       " ('voice', 1),\n",
       " ('puff', 1),\n",
       " ('Smoke', 1),\n",
       " ('evidence', 1),\n",
       " ('mouth', 1),\n",
       " ('Nose', 1),\n",
       " ('accessory', 1),\n",
       " ('selling', 1),\n",
       " ('degree', 1),\n",
       " ('health', 1),\n",
       " ('satisfaction', 1),\n",
       " ('change', 1),\n",
       " ('menthol', 1),\n",
       " ('cold', 1),\n",
       " ('holiday', 1),\n",
       " ('carton', 1),\n",
       " ('way', 1),\n",
       " ('mile', 1),\n",
       " ('lot', 1),\n",
       " ('heart', 1),\n",
       " ('kind', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN' #NNS, NNP, NNPS\n",
    "targetCounts = {}\n",
    "for entry in smokeDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common nouns is **Throat**. Nouns appeared in advertising slogans also includes some other words related to health: doctor, organs, digestion. Also there are words related to cigarette brands (e.g. Lucky). Positive nouns in this corpus are protection, pleasure, mildness, vacation, wind,refreshes, while a little amount of negative nouns also exist like irritation; In addition, advertising slogans mentions nouns related to marketing point and products' feature, like flavor, taste and toast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top verbs (VB, VBD, VBG, VBN, VBP, VBZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('get', 3),\n",
       " ('prevent', 2),\n",
       " ('like', 2),\n",
       " ('Come', 2),\n",
       " ('See', 1),\n",
       " ('tell', 1),\n",
       " ('Give', 1),\n",
       " ('Get', 1),\n",
       " ('enjoy', 1),\n",
       " ('Ask', 1),\n",
       " ('Reach', 1),\n",
       " ('go', 1),\n",
       " ('Switch', 1),\n",
       " ('Blow', 1),\n",
       " ('Slow', 1),\n",
       " ('walk', 1),\n",
       " ('smoke', 1),\n",
       " ('Inhale', 1),\n",
       " ('affect', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB' #VBD, VBG, VBN, VBP, VBZ\n",
    "targetCounts = {}\n",
    "for entry in smokeDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, verbs in advertising slogans are pretty actively and conciously, like get, come, see, tell, enjoy, ask, reach, go, switch, blow, ordered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also look in top adjectives (JJ, JJR, JJS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('other', 3),\n",
       " ('smooth', 2),\n",
       " ('easy', 2),\n",
       " ('single', 1),\n",
       " ('cool', 1),\n",
       " ('healthy', 1),\n",
       " ('psychological', 1),\n",
       " ('dangerous', 1),\n",
       " ('irritating', 1),\n",
       " ('light', 1),\n",
       " ('kind', 1),\n",
       " ('sweet', 1),\n",
       " ('excess', 1),\n",
       " ('harsh', 1),\n",
       " ('extra', 1),\n",
       " ('Sensitive', 1),\n",
       " ('welcome', 1),\n",
       " ('rough', 1),\n",
       " ('Gentle', 1),\n",
       " ('First', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'JJ' #JJR, JJS\n",
    "targetCounts = {}\n",
    "for entry in smokeDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is remarkable that adjectives in advertising slogans are positive: smooth, easy, cool, healthy, light, kind, sweet, gentle; Also, with a goal of marketing, they use some competitive adjectives like finest, best, largest, greatest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our findings above, I am curious about what is the verb that related to the most common noun, \"throat\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prevent'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'VB'\n",
    "Word = 'throat'\n",
    "NResults = set()\n",
    "for entry in smokeDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that in advertising the verb go with \"throat\" is **prevent** (e.g prevent throat from damage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I am interest in what adjective modifies the product (cigarettes) in the slogans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'other'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'cigarette'\n",
    "NResults = set()\n",
    "for entry in smokeDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is **other**, which shows that in advertising slogans creators usually compare their product with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'light'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'smoke'\n",
    "NResults = set()\n",
    "for entry in smokeDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjective modifies \"smoke\" is **light**. So we can see that one of the selling point of cigarettes is its lightness of flavor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anti-smoking Data\n",
    "\n",
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antiDF = pandas.read_excel('../data/antismoke.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antiDF['sentences'] = antiDF['slogan-text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antiDF['POS_sents'] = antiDF['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Smoking', 15),\n",
       " ('t', 14),\n",
       " ('smoking', 10),\n",
       " ('health', 10),\n",
       " ('smoke', 7),\n",
       " ('Smoke', 3),\n",
       " ('Tobacco', 3),\n",
       " ('puff', 2),\n",
       " ('life', 2),\n",
       " ('–', 2),\n",
       " ('air', 2),\n",
       " ('money', 2),\n",
       " ('tobacco', 1),\n",
       " ('Put', 1),\n",
       " ('Fool', 1),\n",
       " ('start', 1),\n",
       " ('Cancer', 1),\n",
       " ('pocket', 1),\n",
       " ('breath', 1),\n",
       " ('mouth', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN' #NNS, NNP, NNPS\n",
    "targetCounts = {}\n",
    "for entry in antiDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common nouns in anti-smoking slogans are smoking and smoke. Rather than use the entity (cigarettes) like the advertisments do, anti-smoking slogans choose to focus on the behavior smoking. Similarly, there are also nouns related to health: health, lungs, cancer, breath, mouth, life. However, different from the advertising slogans, nouns in anti-smoking slogans are more negative, like killers, drugs, worries,  wrinkles, fool, butthead, cowards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Be', 4),\n",
       " ('kill', 2),\n",
       " ('choke', 2),\n",
       " ('Save', 2),\n",
       " ('Breathe', 1),\n",
       " ('be', 1),\n",
       " ('let', 1),\n",
       " ('go', 1),\n",
       " ('die', 1),\n",
       " ('clean', 1),\n",
       " ('don', 1),\n",
       " ('’', 1),\n",
       " ('Live', 1),\n",
       " ('Burn', 1),\n",
       " ('Please', 1),\n",
       " ('keep', 1),\n",
       " ('Put', 1),\n",
       " ('save', 1),\n",
       " ('Smoke', 1),\n",
       " ('cause', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'#VBD, VBG, VBN, VBP, VBZ\n",
    "targetCounts = {}\n",
    "for entry in antiDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the advertisement slogans, verbs in the anti-smoking slogans are more negative, such as kill, die, quit, burn, don't, injure and scares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dead', 2),\n",
       " ('Arsenic', 1),\n",
       " ('smart', 1),\n",
       " ('tough', 1),\n",
       " ('stupid', 1),\n",
       " ('my…', 1),\n",
       " ('overcrowded', 1),\n",
       " ('Quit', 1),\n",
       " ('good', 1),\n",
       " ('clean', 1),\n",
       " ('slow', 1),\n",
       " ('painful', 1),\n",
       " ('rich', 1),\n",
       " ('glamorous', 1),\n",
       " ('deep', 1),\n",
       " ('ugly', 1),\n",
       " ('cool', 1),\n",
       " ('wacko', 1),\n",
       " ('much', 1),\n",
       " ('ash', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'JJ'#JJR, JJS\n",
    "targetCounts = {}\n",
    "for entry in antiDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjectives in anti-smoking slogans are more negative, like dead, tough, stupid, painful and ugly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that **health** appears in both kinds of slogans, let's see what verb go with it in the anti-smoking slogans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Choose'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'VB'\n",
    "Word = 'health'\n",
    "NResults = set()\n",
    "for entry in antiDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's see what adjective modifies the top nouns in anti-smoking slogans: **smoking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Quit'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'smoking'\n",
    "NResults = set()\n",
    "for entry in antiDF['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's **quit** smoking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform NER on a (modest) subset of your corpus of interest. List all of the different kinds of entities tagged? What does their distribution suggest about the focus of your corpus? For a subset of your corpus, tally at least one type of named entity and calculate the Precision, Recall and F-score for the NER classification just performed (using your own hand-codings as \"ground truth\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advertisement Slogans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smokeDF['classified_sents'] = smokeDF['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[(More, O), (doctors, O), (smoke, O), (Camels...\n",
       "1     [[(Not, O), (one, O), (single, O), (case, O), ...\n",
       "2     [[(See, O), (how, O), (your, O), (throat, O), ...\n",
       "3     [[(Your, O), (``, O), (T-Zone, O), ('', O), (w...\n",
       "4     [[(For, O), (digestions, O), (sake, O), (,, O)...\n",
       "5     [[(Give, O), (your, O), (throat, O), (a, O), (...\n",
       "6     [[(They, O), (’, O), (re, O), (smooth, O), (an...\n",
       "7     [[(It, O), (takes, O), (healthy, O), (nerves, ...\n",
       "8     [[(Camels, O), (never, O), (get, O), (on, O), ...\n",
       "9     [[(They, O), (don, O), (’, O), (t, O), (get, O...\n",
       "10    [[(Fatigued, O), (?, O)], [(Get, O), (a, O), (...\n",
       "11    [[(Camels, O), (agree, O), (with, O), (your, O...\n",
       "12    [[(It, O), (’, O), (s, O), (a, O), (psychologi...\n",
       "13    [[(Ivory, O), (tips, O), (protect, O), (the, O...\n",
       "14    [[(Gee, O), (,, O), (Mommy, PERSON), (you, O),...\n",
       "15    [[(Refreshes, O), (as, O), (you, O), (smoke, O)]]\n",
       "16    [[(It, O), (’, O), (s, O), (toasted, O), (., O)]]\n",
       "17    [[(No, O), (Throat, O), (Irritation—No, O), (c...\n",
       "18    [[(Toasting, O), (removes, O), (dangerous, O),...\n",
       "19    [[(The, O), (finest, O), (flavor, O), (and, O)...\n",
       "20    [[(11,105, O), (doctors, O), (say, O), (Lucky,...\n",
       "21    [[(20,679, O), (Physicians, O), (say, O), (Luc...\n",
       "22    [[(Ask, O), (your, O), (doctor, O), (about, O)...\n",
       "23    [[(7, O), (out, O), (of, O), (10, O), (inhale,...\n",
       "24    [[(Luckies, O), (are, O), (easy, O), (on, O), ...\n",
       "25    [[(Luckies, O), (are, O), (always, O), (kind, ...\n",
       "26    [[(Reach, O), (for, O), (a, O), (Lucky, O), (i...\n",
       "27    [[(I, O), (light, O), (a, O), (Lucky, O), (and...\n",
       "28    [[(No, O), (excess, O), (weight, O), (,, O), (...\n",
       "29    [[(Don, O), (’, O), (t, O), (rasp, O), (your, ...\n",
       "30    [[(The, O), (extra, O), (protection, O), (to, ...\n",
       "31    [[(I, O), (protect, O), (my, O), (voice, O), (...\n",
       "32    [[(Sensitive, O), (throats, O), (welcome, O), ...\n",
       "33    [[(There, O), (is, O), (never, O), (a, O), (ro...\n",
       "34    [[(Smoke, O), (to, O), (your, O), (throat, O),...\n",
       "35    [[(Gentle, O), (on, O), (your, O), (throat, O)...\n",
       "36    [[(Chesterfield, LOCATION), (is, O), (best, O)...\n",
       "37    [[(First, O), (to, O), (present, O), (scientif...\n",
       "38    [[(..., O), (they, O), (leave, O), (a, O), (cl...\n",
       "39    [[(Nose, O), (,, O), (throat, O), (,, O), (and...\n",
       "40    [[(The, O), (largest, O), (selling, O), (cigar...\n",
       "Name: classified_sents, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smokeDF['classified_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 46),\n",
       " ('throat', 19),\n",
       " ('a', 19),\n",
       " ('your', 16),\n",
       " ('to', 15),\n",
       " ('the', 8),\n",
       " (',', 8),\n",
       " ('’', 8),\n",
       " ('my', 8),\n",
       " ('of', 7),\n",
       " ('and', 7),\n",
       " ('smoke', 6),\n",
       " ('on', 6),\n",
       " ('you', 5),\n",
       " ('for', 5),\n",
       " ('with', 5),\n",
       " ('Lucky', 5),\n",
       " ('Luckies', 5),\n",
       " ('are', 5),\n",
       " ('is', 5),\n",
       " ('Camels', 4),\n",
       " ('cigarette', 4),\n",
       " ('...', 4),\n",
       " ('They', 4),\n",
       " ('get', 4),\n",
       " ('s', 4),\n",
       " ('The', 4),\n",
       " ('I', 4),\n",
       " ('throats', 4),\n",
       " ('Kools', 4),\n",
       " ('other', 3),\n",
       " ('irritation', 3),\n",
       " ('!', 3),\n",
       " ('``', 3),\n",
       " ('taste', 3),\n",
       " ('For', 3),\n",
       " ('It', 3),\n",
       " ('No', 3),\n",
       " ('light', 3),\n",
       " ('protection', 3),\n",
       " ('in', 3),\n",
       " ('doctors', 2),\n",
       " ('any', 2),\n",
       " ('Your', 2),\n",
       " (\"''\", 2),\n",
       " ('will', 2),\n",
       " ('T', 2),\n",
       " ('Throat', 2),\n",
       " ('re', 2),\n",
       " ('smooth', 2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityCounts = {}\n",
    "for entry in smokeDF['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Along with the most common entities in advertising slogans, we can identify one kind of specific entity - brand of cigarettes, like **Luckies, Camel, Kools**. The certain entity behind these mentioned entities are acturally **'cigarettes'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look into those occurring only twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doctors',\n",
       " 'other',\n",
       " 'cigarette',\n",
       " 'T',\n",
       " 'taste',\n",
       " 'Throat',\n",
       " 'They',\n",
       " 'easy',\n",
       " 'nerves',\n",
       " 'never',\n",
       " 'get',\n",
       " 't',\n",
       " 'protect',\n",
       " 'No',\n",
       " 'irritants',\n",
       " 'say',\n",
       " 'inhale',\n",
       " 'smoking']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are any organizations in the slogans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in smokeDF['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mommy', 1)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perCounts = {}\n",
    "for entry in smokeDF['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'PERSON':\n",
    "                continue\n",
    "            elif ent in perCounts:\n",
    "                perCounts[ent] += 1\n",
    "            else:\n",
    "                perCounts[ent] = 1\n",
    "sortedOrgs = sorted(perCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chesterfield', 2), ('America', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LocCounts = {}\n",
    "for entry in smokeDF['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'LOCATION':\n",
    "                continue\n",
    "            elif ent in LocCounts:\n",
    "                LocCounts[ent] += 1\n",
    "            else:\n",
    "                LocCounts[ent] = 1\n",
    "sortedLoc = sorted(LocCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedLoc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are basically no significant person, location and organization entities in our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anti-smoking Slogans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antiDF['classified_sents'] = antiDF['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[(Arsenic, O), (kills, O), (if, O), (you, O),...\n",
       "1     [[(Be, O), (A, O), (Fighter, O), (;, O), (Put,...\n",
       "2     [[(Be, O), (Cool, O), (–, O), (Don, O), (’, O)...\n",
       "3     [[(Be, O), (smart, O), (don, O), (’, O), (t, O...\n",
       "4     [[(Breathe, O), (healthily, O), (,, O), (live,...\n",
       "5     [[(Bullying, O), (is, O), (like, O), (smoking,...\n",
       "6     [[(Cancer, O), (cures, O), (smoking, O), (., O)]]\n",
       "7     [[(Cigarettes, O), (are, O), (killers, O), (th...\n",
       "8     [[(Cigarettes, O), (burn, O), (holes, O), (in,...\n",
       "9     [[(Cigarettes, O), (:, O), (You, O), (take, O)...\n",
       "10    [[(Did, O), (you, O), (know, O), (your, O), (m...\n",
       "11    [[(Don, O), (’, O), (t, O), (be, O), (a, O), (...\n",
       "12    [[(Don, O), (’, O), (t, O), (let, O), (your, O...\n",
       "13    [[(Don, O), (’, O), (t, O), (pout, O), (,, O),...\n",
       "14    [[(Don, PERSON), (’, PERSON), (t, O), (puff, O...\n",
       "15    [[(Don, O), (’, O), (t, O), (smoke, O), (–, O)...\n",
       "16    [[(Don, O), (’, O), (t, O), (smoke, O), (you, ...\n",
       "17    [[(Don, O), (’, O), (t, O), (smoke, O), (,, O)...\n",
       "18    [[(Everyone, O), (has, O), (a, O), (right, O),...\n",
       "19    [[(Hang, O), (tough, O), (,, O), (don, O), (’,...\n",
       "20    [[(I, O), (like, O), (smoking, O), (., O)], [(...\n",
       "21    [[(I, O), (quit, O), (because, O), (my, O), (k...\n",
       "22    [[(If, O), (you, O), (can, O), (’, O), (t, O),...\n",
       "23    [[(Lions, O), (,, O), (and, O), (Tigers, ORGAN...\n",
       "24    [[(Live, O), (it, O), (or, O), (Burn, O), (it…...\n",
       "25    [[(Please, O), (keep, O), (smoking, O), (., O)...\n",
       "26    [[(Put, O), (it, O), (out, O), (before, O), (i...\n",
       "27    [[(Quit, O), (smoking, O), (before, O), (smoki...\n",
       "28    [[(Quit, O), (smoking, O), (., O)], [(It, O), ...\n",
       "29    [[(Quitting, O), (has, O), (never, O), (felt, ...\n",
       "                            ...                        \n",
       "38    [[(Smokers, O), (are, O), (jokers, O), (–, O),...\n",
       "39    [[(Smoking, O), (–, O), (Suicide, O), (for, O)...\n",
       "40    [[(Smoking, O), (batters, O), (your, O), (heal...\n",
       "41    [[(Smoking, O), (cigarettes, O), (is, O), (so,...\n",
       "42    [[(Smoking, O), (fail, O), (your, O), (Health,...\n",
       "43    [[(Smoking, O), (injures, O), (your, O), (heal...\n",
       "44    [[(Smoking, O), (is, O), (like, O), (paying, O...\n",
       "45    [[(Smoking, O), (is, O), (one, O), (of, O), (t...\n",
       "46    [[(Smoking, O), (is, O), (very, O), (glamorous...\n",
       "47    [[(Smoking, O), (makes, O), (you, O), (ugly, O...\n",
       "48    [[(Smoking, O), (pack-up, O), (your, O), (heal...\n",
       "49    [[(Smoking, O), (prohibited, O), (., O)], [(Pe...\n",
       "50    [[(Smoking, O), (scares, O), (your, O), (Healt...\n",
       "51    [[(Smoking, O), (slice, O), (your, O), (Health...\n",
       "52                [[(Smoking, O), (Stinks, O), (!, O)]]\n",
       "53    [[(Smoking, O), (the, O), (dope, O), (won, O),...\n",
       "54    [[(Smoking, O), (torture, O), (your, O), (Heal...\n",
       "55         [[(Smoking…a, O), (grave, O), (mistake, O)]]\n",
       "56    [[(Tar, O), (the, O), (roads, O), (,, O), (not...\n",
       "57    [[(The, O), (cigarette, O), (does, O), (the, O...\n",
       "58    [[(The, O), (Cigarette, O), (is, O), (dead, O)...\n",
       "59    [[(Think, O), (smoking, O), (is, O), (cool, O)...\n",
       "60    [[(Tobacco, O), (companies, O), (kill, O), (th...\n",
       "61                [[(Tobacco, O), (is, O), (wacko, O)]]\n",
       "62    [[(Tobacco, O), (or, O), (Health, O), (:, O), ...\n",
       "63    [[(Too, O), (much, O), (smoke, O), (will, O), ...\n",
       "64           [[(Trash, O), (the, O), (ash, O), (., O)]]\n",
       "65    [[(Who, O), (says, O), (being, O), (a, O), (qu...\n",
       "66    [[(You, O), (smoke, O), (,, O), (you, O), (die...\n",
       "67    [[(Your, O), (money, O), (going, O), (up, O), ...\n",
       "Name: classified_sents, Length: 68, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antiDF['classified_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 70),\n",
       " ('your', 23),\n",
       " ('Smoking', 19),\n",
       " ('’', 18),\n",
       " ('you', 17),\n",
       " ('t', 14),\n",
       " (',', 13),\n",
       " ('is', 11),\n",
       " ('smoking', 10),\n",
       " ('health', 10),\n",
       " ('smoke', 9),\n",
       " ('Don', 9),\n",
       " ('a', 9),\n",
       " ('it', 8),\n",
       " ('No-Smoking', 7),\n",
       " ('the', 7),\n",
       " ('–', 6),\n",
       " ('!', 6),\n",
       " ('and', 6),\n",
       " ('Health', 6),\n",
       " ('kills', 4),\n",
       " ('Be', 4),\n",
       " ('like', 4),\n",
       " ('are', 4),\n",
       " ('in', 4),\n",
       " ('Smoke', 4),\n",
       " ('if', 3),\n",
       " (';', 3),\n",
       " ('The', 3),\n",
       " ('don', 3),\n",
       " ('can', 3),\n",
       " ('kill', 3),\n",
       " ('Cigarettes', 3),\n",
       " ('away', 3),\n",
       " ('?', 3),\n",
       " ('out', 3),\n",
       " ('to', 3),\n",
       " ('will', 3),\n",
       " ('not', 3),\n",
       " ('of', 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityCounts = {}\n",
    "for entry in antiDF['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tigers', 1), ('Choose', 1), ('Health', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in antiDF['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Don', 1), ('’', 1), ('Pezing', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PerCounts = {}\n",
    "for entry in antiDF['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'PERSON':\n",
    "                continue\n",
    "            elif ent in PerCounts:\n",
    "                PerCounts[ent] += 1\n",
    "            else:\n",
    "                PerCounts[ent] = 1\n",
    "sortedPers = sorted(PerCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedPers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LocCounts = {}\n",
    "for entry in antiDF['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'LOCATION':\n",
    "                continue\n",
    "            elif ent in LocCounts:\n",
    "                LocCounts[ent] += 1\n",
    "            else:\n",
    "                LocCounts[ent] = 1\n",
    "sortedLoc = sorted(LocCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedLoc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating precision, recall and f-measure for the name entities that was classfied here, according to my hand coding classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/czd/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/czd/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5, 0.5, None)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = np.array(['chesterfield', 'America','Mommy'])\n",
    "y_pred = np.array(['college', 'America','Mommy'])\n",
    "precision_recall_fscore_support(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, parse a (modest) subset of your corpus of interest. How deep are the phrase structure and dependency parse trees nested? How does parse depth relate to perceived sentence complexity? What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these entities are perceived to play in the social world inscribed by your texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advertisement Slogans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Give your throat a vacation!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smokeDepParse = list(stanford.depParser.parse_sents(smokeDF['sentences'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give your throat a vacation ...\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 0\n",
    "print(' '.join(smokeDF['sentences'][5][targetSentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"178pt\" height=\"302pt\"\n",
       " viewBox=\"0.00 0.00 177.86 302.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 298)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-298 173.8623,-298 173.8623,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"80.2397\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"80.2397\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Give)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M80.2397,-257.7616C80.2397,-246.3597 80.2397,-231.4342 80.2397,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"83.7398,-218.2121 80.2397,-208.2121 76.7398,-218.2121 83.7398,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"91.5156\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"34.2397\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (throat)</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M70.4843,-171.7616C64.2027,-160.0176 55.9215,-144.5355 48.8607,-131.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"51.7839,-129.3792 43.9811,-122.2121 45.6114,-132.6808 51.7839,-129.3792\"/>\n",
       "<text text-anchor=\"middle\" x=\"70.3467\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"128.2397\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (vacation)</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M90.4193,-171.7616C97.0377,-159.9036 105.7833,-144.2345 113.1973,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.2573,-132.6499 118.0749,-122.2121 110.1449,-129.2383 116.2573,-132.6499\"/>\n",
       "<text text-anchor=\"middle\" x=\"119.6846\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"34.2397\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (your)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M34.2397,-85.7616C34.2397,-74.3597 34.2397,-59.4342 34.2397,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"37.7398,-46.2121 34.2397,-36.2121 30.7398,-46.2121 37.7398,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"64.5776\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"128.2397\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (a)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M128.2397,-85.7616C128.2397,-74.3597 128.2397,-59.4342 128.2397,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"131.7398,-46.2121 128.2397,-36.2121 124.7398,-46.2121 131.7398,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"136.7915\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11cf43390>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(smokeDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Come to Marlboro Country.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smokeDepParse = list(stanford.depParser.parse_sents(smokeDF['sentences'][53]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come to Marlboro Country\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 0\n",
    "print(' '.join(smokeDF['sentences'][53][targetSentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"256pt\" height=\"216pt\"\n",
       " viewBox=\"0.00 0.00 255.98 216.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 212)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-212 251.9829,-212 251.9829,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"114.6328\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"114.6328\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (Country)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M114.6328,-171.7616C114.6328,-160.3597 114.6328,-145.4342 114.6328,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.1329,-132.2121 114.6328,-122.2121 111.1329,-132.2121 118.1329,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.9087\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"34.6328\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Come)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M97.6669,-85.7616C86.318,-73.5615 71.2164,-57.3273 58.6399,-43.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"60.948,-41.1501 51.5743,-36.2121 55.8227,-45.9179 60.948,-41.1501\"/>\n",
       "<text text-anchor=\"middle\" x=\"91.6777\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"114.6328\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (to)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M114.6328,-85.7616C114.6328,-74.3597 114.6328,-59.4342 114.6328,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.1329,-46.2121 114.6328,-36.2121 111.1329,-46.2121 118.1329,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"126.6777\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"203.6328\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (Marlboro)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.5074,-85.7616C146.2511,-73.4475 163.248,-57.0235 177.3165,-43.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"180.0263,-45.6779 184.7854,-36.2121 175.1621,-40.644 180.0263,-45.6779\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.1846\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11d1bde48>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(smokeDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Gee, Mommy you sure enjoy your Marlboro.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smokeDepParse = list(stanford.depParser.parse_sents(smokeDF['sentences'][14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gee , Mommy you sure enjoy your Marlboros\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 0\n",
    "print(' '.join(smokeDF['sentences'][14][targetSentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"296pt\" height=\"388pt\"\n",
       " viewBox=\"0.00 0.00 295.70 388.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 384)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-384 291.7017,-384 291.7017,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"132.0259\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"132.0259\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Gee)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M132.0259,-343.7616C132.0259,-332.3597 132.0259,-317.4342 132.0259,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"135.526,-304.2121 132.0259,-294.2121 128.526,-304.2121 135.526,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.3018\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"132.0259\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (enjoy)</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;6 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M132.0259,-257.7616C132.0259,-246.3597 132.0259,-231.4342 132.0259,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"135.526,-218.2121 132.0259,-208.2121 128.526,-218.2121 135.526,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.0776\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">xcomp</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"42.0259\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (Mommy)</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>6&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M112.9392,-171.7616C100.0523,-159.4475 82.8645,-143.0235 68.6379,-129.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"70.733,-126.5902 61.0851,-122.2121 65.897,-131.6512 70.733,-126.5902\"/>\n",
       "<text text-anchor=\"middle\" x=\"108.1948\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"132.0259\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (sure)</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>6&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M132.0259,-171.7616C132.0259,-160.3597 132.0259,-145.4342 132.0259,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"135.526,-132.2121 132.0259,-122.2121 128.526,-132.2121 135.526,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"142.1328\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"227.0259\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (Marlboros)</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;8 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M152.1729,-171.7616C165.9017,-159.3335 184.2548,-142.719 199.3517,-129.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"201.8433,-131.518 206.9079,-122.2121 197.1454,-126.3286 201.8433,-131.518\"/>\n",
       "<text text-anchor=\"middle\" x=\"198.4707\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"42.0259\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (you)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M42.0259,-85.7616C42.0259,-74.3597 42.0259,-59.4342 42.0259,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"45.526,-46.2121 42.0259,-36.2121 38.526,-46.2121 45.526,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"52.1328\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"227.0259\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (your)</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>8&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M227.0259,-85.7616C227.0259,-74.3597 227.0259,-59.4342 227.0259,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.526,-46.2121 227.0259,-36.2121 223.526,-46.2121 230.526,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"257.3638\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11d1bd240>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(smokeDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anti-smoking Slogans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Be a fighter; Put down your lighter.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antiDepParse = list(stanford.depParser.parse_sents(antiDF['sentences'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be A Fighter ; Put Down The Lighter .\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 0\n",
    "print(' '.join(antiDF['sentences'][1][targetSentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"249pt\" height=\"388pt\"\n",
       " viewBox=\"0.00 0.00 248.79 388.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 384)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-384 244.7949,-384 244.7949,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (Fighter)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;3 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M99,-343.7616C99,-332.3597 99,-317.4342 99,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"102.5001,-304.2121 99,-294.2121 95.5001,-304.2121 102.5001,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"110.2759\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Be)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M83.7307,-257.7616C73.6121,-245.6756 60.179,-229.6304 48.9243,-216.1874\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"51.3504,-213.6329 42.2473,-208.2121 45.9831,-218.1265 51.3504,-213.6329\"/>\n",
       "<text text-anchor=\"middle\" x=\"78.1069\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (A)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M99,-257.7616C99,-246.3597 99,-231.4342 99,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"102.5001,-218.2121 99,-208.2121 95.5001,-218.2121 102.5001,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"107.5518\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (Put)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M114.2693,-257.7616C124.3879,-245.6756 137.821,-229.6304 149.0757,-216.1874\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"152.0169,-218.1265 155.7527,-208.2121 146.6496,-213.6329 152.0169,-218.1265\"/>\n",
       "<text text-anchor=\"middle\" x=\"165.2656\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">parataxis</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (Lighter)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;8 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M171,-171.7616C171,-160.3597 171,-145.4342 171,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"174.5001,-132.2121 171,-122.2121 167.5001,-132.2121 174.5001,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.1069\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"130\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (Down)</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>8&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M162.305,-85.7616C156.7061,-74.0176 149.3251,-58.5355 143.0317,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"146.1453,-43.7326 138.6825,-36.2121 139.8266,-46.745 146.1453,-43.7326\"/>\n",
       "<text text-anchor=\"middle\" x=\"165.0449\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (The)</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>8&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M179.695,-85.7616C185.2939,-74.0176 192.6749,-58.5355 198.9683,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"202.1734,-46.745 203.3175,-36.2121 195.8547,-43.7326 202.1734,-46.745\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.5518\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11cf2de48>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(antiDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Smoking batters your health, no-smoking betters your health.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antiDepParse = list(stanford.depParser.parse_sents(antiDF['sentences'][40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoking batters your health .\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 0\n",
    "print(' '.join(antiDF['sentences'][40][targetSentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"208pt\" height=\"302pt\"\n",
       " viewBox=\"0.00 0.00 207.87 302.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 298)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-298 203.8706,-298 203.8706,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"80.1948\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"80.1948\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (batters)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M80.1948,-257.7616C80.1948,-246.3597 80.1948,-231.4342 80.1948,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"83.6949,-218.2121 80.1948,-208.2121 76.6949,-218.2121 83.6949,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"91.4707\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"43.1948\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Smoking)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M72.3481,-171.7616C67.3445,-160.1316 60.7637,-144.8357 55.1209,-131.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"58.1975,-130.0148 51.0303,-122.2121 51.7673,-132.7813 58.1975,-130.0148\"/>\n",
       "<text text-anchor=\"middle\" x=\"78.3638\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"139.1948\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (health)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M92.7072,-171.7616C100.9206,-159.7896 111.7991,-143.9328 120.9676,-130.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"123.9294,-132.4381 126.7005,-122.2121 118.1572,-128.4781 123.9294,-132.4381\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.6396\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"139.1948\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (your)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M139.1948,-85.7616C139.1948,-74.3597 139.1948,-59.4342 139.1948,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"142.6949,-46.2121 139.1948,-36.2121 135.6949,-46.2121 142.6949,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5327\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11cf2d898>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(antiDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No-Smoking Betters your health .\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 1\n",
    "print(' '.join(antiDF['sentences'][40][targetSentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"230pt\" height=\"302pt\"\n",
       " viewBox=\"0.00 0.00 229.76 302.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 298)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-298 225.7568,-298 225.7568,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"94.0811\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"94.0811\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (Betters)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M94.0811,-257.7616C94.0811,-246.3597 94.0811,-231.4342 94.0811,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.5812,-218.2121 94.0811,-208.2121 90.5812,-218.2121 97.5812,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.3569\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"54.0811\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (No&#45;Smoking)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M85.5981,-171.7616C80.1358,-160.0176 72.9348,-144.5355 66.7949,-131.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"69.9427,-129.8032 62.5518,-122.2121 63.5956,-132.7554 69.9427,-129.8032\"/>\n",
       "<text text-anchor=\"middle\" x=\"91.25\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"161.0811\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (health)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M108.29,-171.7616C117.7059,-159.6756 130.2062,-143.6304 140.6793,-130.1874\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"143.5078,-132.2517 146.8926,-122.2121 137.9858,-127.9497 143.5078,-132.2517\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.5259\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"161.0811\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (your)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M161.0811,-85.7616C161.0811,-74.3597 161.0811,-59.4342 161.0811,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"164.5812,-46.2121 161.0811,-36.2121 157.5812,-46.2121 164.5812,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"191.4189\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11d1da208>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(antiDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Lions, and Tigers and Bears oh my… Drinking and Smoking and Drugs? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antiDepParse = list(stanford.depParser.parse_sents(antiDF['sentences'][23]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lions , and Tigers and Bears oh my… Drinking and Smoking and Drugs ?\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 0\n",
    "print(' '.join(antiDF['sentences'][23][targetSentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"471pt\" height=\"388pt\"\n",
       " viewBox=\"0.00 0.00 470.54 388.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 384)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-384 466.5415,-384 466.5415,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"123.019\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"123.019\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Lions)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M123.019,-343.7616C123.019,-332.3597 123.019,-317.4342 123.019,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"126.5191,-304.2121 123.019,-294.2121 119.5191,-304.2121 126.5191,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"134.2949\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"28.019\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (and)</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M102.872,-257.7616C89.1432,-245.3335 70.7901,-228.719 55.6932,-215.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"57.8995,-212.3286 48.1371,-208.2121 53.2017,-217.518 57.8995,-212.3286\"/>\n",
       "<text text-anchor=\"middle\" x=\"88.2329\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"106.019\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (and)</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;10 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M119.4138,-257.7616C117.1374,-246.2456 114.1504,-231.1353 111.575,-218.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"114.992,-217.3435 109.6191,-208.2121 108.1248,-218.701 114.992,-217.3435\"/>\n",
       "<text text-anchor=\"middle\" x=\"122.2329\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"199.019\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (Drinking)</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;9 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M139.1367,-257.7616C149.9182,-245.5615 164.2647,-229.3273 176.2123,-215.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"178.9253,-218.0231 182.9246,-208.2121 173.68,-213.3877 178.9253,-218.0231\"/>\n",
       "<text text-anchor=\"middle\" x=\"178.0708\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"336.019\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (Smoking)</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;11 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M156.7695,-262.373C190.2931,-248.8377 242.2384,-227.8645 281.5963,-211.9735\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.3502,-215.0399 291.3125,-208.0505 280.7294,-208.549 283.3502,-215.0399\"/>\n",
       "<text text-anchor=\"middle\" x=\"255.0708\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"89.019\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (Tigers)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;4 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>9&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M155.5798,-174.8614C133.59,-166.8702 110.7288,-157.925 106.9155,-154 101.0431,-147.9554 97.0826,-139.8529 94.4174,-131.9326\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.7505,-130.8552 91.6878,-122.1665 91.0088,-132.7395 97.7505,-130.8552\"/>\n",
       "<text text-anchor=\"middle\" x=\"136.5708\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"171.019\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (oh)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;7 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>9&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M181.9346,-171.8744C177.8836,-166.5115 174.1061,-160.3614 171.9155,-154 169.5605,-147.161 168.6434,-139.4778 168.4776,-132.2242\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"171.9804,-132.1244 168.6926,-122.0526 164.982,-131.9764 171.9804,-132.1244\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.5708\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"253.019\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (my…)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M217.1398,-171.8385C222.0212,-166.3823 227.0445,-160.1931 231.019,-154 235.4354,-147.1186 239.3724,-139.2009 242.652,-131.7256\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"245.9723,-132.8556 246.5697,-122.2776 239.5062,-130.1743 245.9723,-132.8556\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.5708\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"336.019\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (and)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M336.019,-171.7616C336.019,-160.3597 336.019,-145.4342 336.019,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"339.5191,-132.2121 336.019,-122.2121 332.5191,-132.2121 339.5191,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"342.2329\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"424.019\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (Drugs)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;13 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M354.6816,-171.7616C367.2821,-159.4475 384.088,-143.0235 397.9984,-129.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"400.6778,-131.7046 405.3834,-122.2121 395.7852,-126.6983 400.6778,-131.7046\"/>\n",
       "<text text-anchor=\"middle\" x=\"397.0708\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"49.019\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (and)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M80.5361,-85.7616C75.0737,-74.0176 67.8728,-58.5355 61.7329,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.8807,-43.8032 57.4898,-36.2121 58.5336,-46.7554 64.8807,-43.8032\"/>\n",
       "<text text-anchor=\"middle\" x=\"78.2329\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"129.019\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (Bears)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M97.502,-85.7616C102.9643,-74.0176 110.1653,-58.5355 116.3051,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"119.5045,-46.7554 120.5483,-36.2121 113.1574,-43.8032 119.5045,-46.7554\"/>\n",
       "<text text-anchor=\"middle\" x=\"124.0708\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11cf2d390>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(antiDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our dependency parsing structure tree shown above, I concludes five siginificant features of advertising slogans and anti-smoking slogans.\n",
    "\n",
    "1) The most significant feature of both advertising slogans and anti-smoking slogans is that they use very **simple verb phrase** to complete a sentences, which makes most of the slogans imperative sentences, like *\"give your throat a vocation\"* and *\"Be a fighter, put down the lighter\"*. This sentence structure reveals features of slogan: strong emotion, active, easy to read and memory.\n",
    "\n",
    "2) For adversiting slogans, another feature is that it tries to **build a relation between products and target costumers**. The famous slogan of *\"Gee, Mommy you sure enjoy your Marlboro\"* is an example. In this sentence, the nominal subject **Mommy** is connected with the direct object **Marlboro** with the movement **enjoy**. Since this slogan is created under the background that Marlboro had beed repositioned to appeal new generation mothers after WW2, we can see that the it intentionally built a relation between its target population and the product.\n",
    "\n",
    "3) For anti-smoking slogans, a feature that different from advertising slogans is that it tries to combine two or more simple verb phrase into one slogan, to **make a comparison between smoking and non-smoking**, like the *\"Smoking batters your health, no-smoking betters your health\"*. In such sentences, the verb and adjective go with smoking are always negative, while the ones go with non-smoking are positive.\n",
    "\n",
    "4) In these two kinds of slogans, we can see that different verbs are going with the same subject - smoking (or cigarettes). In our examples, in advertising slogans, verbs related to smoking or cigarettes are enjoy, give and betters, while verbs in anti-smoking are put down and batters. The former is positive, the latter is negative.\n",
    "\n",
    "5) Collocation of nouns in these two kinds of slogans is actually making analogies of smoking. Therefore, we can observe the attitude towards smoking in different slogans. In advertising slogans, the noun co-occuring with (i.e. analogies to) smoking is vocation, while in anti-smoking slogans, the noun co-occuring with smoking is \"drinking\", \"drugs\", \"tigers\", etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">How would you extract relevant information about the Trayvon Martin sentence directly from the dependency parse (above)? Code an example here. (For instance, what compound nouns show up with what verb phrases within the sentence?) How could these approaches inform your research project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"997pt\" height=\"560pt\"\n",
       " viewBox=\"0.00 0.00 996.96 560.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 556)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-556 992.9575,-556 992.9575,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (American)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;7 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M235.3833,-515.7616C235.3833,-504.3597 235.3833,-489.4342 235.3833,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8834,-476.2121 235.3833,-466.2121 231.8834,-476.2121 238.8834,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"246.6592\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"77.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (Martin)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>7&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5259,-429.8504C183.9641,-424.3453 171.399,-418.1273 160.0454,-412 144.9107,-403.8322 128.6384,-394.1858 114.5678,-385.5556\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.0198,-382.3384 105.6735,-380.0569 112.3388,-388.2925 116.0198,-382.3384\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5522\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"161.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (was)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>7&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M219.6898,-429.7616C209.1921,-417.5615 195.2231,-401.3273 183.5899,-387.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.2297,-385.5094 177.0542,-380.2121 180.9236,-390.0751 186.2297,-385.5094\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.4902\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (an)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M235.3833,-429.7616C235.3833,-418.3597 235.3833,-403.4342 235.3833,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8834,-390.2121 235.3833,-380.2121 231.8834,-390.2121 238.8834,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"319.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (African)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M253.1975,-429.7616C265.2253,-417.4475 281.2673,-401.0235 294.5454,-387.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"297.1111,-389.8115 301.5947,-380.2121 292.1035,-384.9203 297.1111,-389.8115\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"421.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (Gardens)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M281.035,-436.267C301.523,-430.2297 325.6866,-422.0184 346.3833,-412 361.1468,-404.8536 376.3934,-395.1776 389.2526,-386.2414\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"391.6132,-388.8578 397.7385,-380.213 387.5592,-383.1511 391.6132,-388.8578\"/>\n",
       "<text text-anchor=\"middle\" x=\"386.3281\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"41.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Trayvon)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"146.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (Benjamin)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M69.7486,-343.7616C64.8803,-332.1316 58.4773,-316.8357 52.987,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.097,-302.085 49.007,-294.2121 49.6399,-304.788 56.097,-302.085\"/>\n",
       "<text text-anchor=\"middle\" x=\"91.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M105.7905,-343.9005C112.4839,-338.7035 119.1771,-332.628 124.3833,-326 129.5733,-319.3927 133.8059,-311.3704 137.1222,-303.7037\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.4118,-304.9025 140.8359,-294.3161 133.9027,-302.3275 140.4118,-304.9025\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"274.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>10&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M376.5212,-344.6694C364.1537,-339.2688 350.9417,-332.9006 339.2935,-326 326.8103,-318.6047 313.97,-309.165 303.0389,-300.4839\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"305.0617,-297.6182 295.0895,-294.0398 300.6536,-303.0559 305.0617,-297.6182\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.4282\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"360.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (Miami)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>10&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M391.2568,-343.6347C384.9071,-338.5898 378.7774,-332.6501 374.2798,-326 369.9075,-319.5352 366.903,-311.7204 364.8413,-304.2096\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"368.2186,-303.2775 362.5744,-294.3112 361.3952,-304.8402 368.2186,-303.2775\"/>\n",
       "<text text-anchor=\"middle\" x=\"403.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"456.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (Florida)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M428.8059,-343.7616C433.539,-332.1316 439.7641,-316.8357 445.1019,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"448.4436,-304.7938 448.9714,-294.2121 441.96,-302.1551 448.4436,-304.7938\"/>\n",
       "<text text-anchor=\"middle\" x=\"457.7144\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"549.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (shot)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;23 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M450.562,-343.9652C459.5457,-338.3073 469.4264,-331.9769 478.3833,-326 490.7955,-317.7174 504.223,-308.3654 516.0254,-300.0089\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"518.1726,-302.7768 524.2944,-294.1295 514.1163,-297.0718 518.1726,-302.7768\"/>\n",
       "<text text-anchor=\"middle\" x=\"520.9214\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"341.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (who)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>23&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1602,-260.9769C513.2216,-259.8878 510.2679,-258.8756 507.3833,-258 465.6572,-245.3346 451.0323,-257.1374 410.9351,-240 396.1578,-233.6843 381.463,-223.7913 369.4142,-214.4665\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"371.5209,-211.6696 361.5269,-208.1526 367.1463,-217.1344 371.5209,-211.6696\"/>\n",
       "<text text-anchor=\"middle\" x=\"438.6074\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubjpass</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"423.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (old)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;19 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>23&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1838,-257.9822C506.6151,-252.48 496.2767,-246.2305 487.0659,-240 475.5821,-232.2319 463.494,-223.0275 452.972,-214.6466\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.892,-211.6986 444.9108,-208.1451 450.4974,-217.1473 454.892,-211.6986\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.542\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"504.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (was)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;21 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>23&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M538.2032,-257.8722C534.8316,-252.2105 531.1955,-245.9014 528.0591,-240 524.2785,-232.8864 520.4337,-225.0669 516.9726,-217.7676\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"519.9328,-215.8341 512.5329,-208.2517 513.5892,-218.7937 519.9328,-215.8341\"/>\n",
       "<text text-anchor=\"middle\" x=\"550.5454\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">auxpass</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"594.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">22 (fatally)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;22 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>23&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M561.88,-257.643C565.455,-252.0813 569.2329,-245.8848 572.3833,-240 576.1375,-232.9872 579.7933,-225.2024 583.0121,-217.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"586.3752,-218.9423 587.0989,-208.3722 579.9415,-216.1839 586.3752,-218.9423\"/>\n",
       "<text text-anchor=\"middle\" x=\"601.9351\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"707.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (Zimmerman)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;26 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>23&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M582.5441,-262.0972C596.8621,-255.7864 613.6686,-247.961 628.3833,-240 643.1389,-232.0169 658.8574,-222.3295 672.3559,-213.6153\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"674.4115,-216.4533 680.8734,-208.0572 670.586,-210.5911 674.4115,-216.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"667.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"856.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (Florida)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;36 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>23&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M582.6464,-267.6451C610.6488,-260.5344 651.722,-249.924 687.3833,-240 726.7862,-229.0348 771.1175,-215.8883 804.4873,-205.8282\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"805.9136,-209.0536 814.474,-202.811 803.889,-202.3527 805.9136,-209.0536\"/>\n",
       "<text text-anchor=\"middle\" x=\"750.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"391.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (at)</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (17)</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (years)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;17 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>18&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M472.3833,-85.7616C472.3833,-74.3597 472.3833,-59.4342 472.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"475.8834,-46.2121 472.3833,-36.2121 468.8834,-46.2121 475.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"497.2729\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nummod</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;16 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>19&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M416.5969,-171.7616C412.2695,-160.1316 406.578,-144.8357 401.6977,-131.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"404.9276,-130.3637 398.1599,-122.2121 398.367,-132.8049 404.9276,-130.3637\"/>\n",
       "<text text-anchor=\"middle\" x=\"421.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M433.7749,-171.7616C440.5313,-159.9036 449.459,-144.2345 457.0275,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"460.0971,-132.6334 462.0066,-122.2121 454.0151,-129.1681 460.0971,-132.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"488.7178\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:npmod</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"567.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (by)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;24 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>26&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M668.1869,-171.9496C657.328,-166.5419 645.6763,-160.3501 635.2935,-154 622.6297,-146.2549 609.3446,-136.8017 597.9168,-128.2084\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"600.0333,-125.4208 589.9593,-122.1398 595.7884,-130.9869 600.0333,-125.4208\"/>\n",
       "<text text-anchor=\"middle\" x=\"647.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"655.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (George)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;25 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>26&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M684.2392,-171.951C678.6851,-166.6739 673.2261,-160.5473 669.2798,-154 665.2509,-147.3157 662.3575,-139.4301 660.2923,-131.9215\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"663.6687,-130.9902 657.9608,-122.0643 656.8567,-132.6015 663.6687,-130.9902\"/>\n",
       "<text text-anchor=\"middle\" x=\"698.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"763.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (volunteer)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;31 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>26&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M719.2595,-171.7616C726.981,-159.9036 737.1841,-144.2345 745.8338,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"749.0005,-132.502 751.5243,-122.2121 743.1345,-128.6822 749.0005,-132.502\"/>\n",
       "<text text-anchor=\"middle\" x=\"755.7144\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"856.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (in)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;33 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>36&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M856.3833,-171.7616C856.3833,-160.3597 856.3833,-145.4342 856.3833,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"859.8834,-132.2121 856.3833,-122.2121 852.8834,-132.2121 859.8834,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"868.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"945.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (Sanford)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;34 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>36&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M875.2579,-171.7616C888.0016,-159.4475 904.9985,-143.0235 919.067,-129.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"921.7767,-131.6779 926.5359,-122.2121 916.9125,-126.644 921.7767,-131.6779\"/>\n",
       "<text text-anchor=\"middle\" x=\"934.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"658.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (a)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;28 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>31&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M741.3719,-85.9716C725.9715,-73.358 705.2148,-56.3573 688.313,-42.5139\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"690.3198,-39.6334 680.3658,-36.0047 685.8843,-45.0488 690.3198,-39.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"725.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"763.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (neighborhood)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M763.3833,-85.7616C763.3833,-74.3597 763.3833,-59.4342 763.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"766.8834,-46.2121 763.3833,-36.2121 759.8834,-46.2121 766.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"792.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"879.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (watch)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M798.3154,-85.9914C807.713,-80.629 817.6789,-74.4499 826.3833,-68 836.2881,-60.6606 846.3323,-51.6634 854.9744,-43.3341\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"857.5027,-45.7565 862.1694,-36.2448 852.5896,-40.7703 857.5027,-45.7565\"/>\n",
       "<text text-anchor=\"middle\" x=\"873.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11d3e7780>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(depParses[3])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the elephant in my pajamas.\n",
      "The quick brown fox jumped over the lazy dog.\n",
      "While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.\n",
      "Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.\n",
      "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\n"
     ]
    }
   ],
   "source": [
    "text = ['I saw the elephant in my pajamas.', 'The quick brown fox jumped over the lazy dog.', 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.', 'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.', 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']\n",
    "tokenized_text = [nltk.word_tokenize(t) for t in text]\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 36.434 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [39.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0095 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/bj/v89n36091c9894hp0l9r5hgm0000gn/T/tmpiaz4ocy6\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>elephant</td>\n",
       "      <td>is in</td>\n",
       "      <td>my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant in my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed stimulus efforts in</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent intervie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview with Wall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "      <td>is in</td>\n",
       "      <td>recent interview with Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview with Wall Street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>recent interview</td>\n",
       "      <td>is with</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was African American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty                      subject                           verb  \\\n",
       "0         1.0                     elephant                          is in   \n",
       "1         1.0                            I                            saw   \n",
       "2         1.0                            I                            saw   \n",
       "3         1.0              quick brown fox                    jumped over   \n",
       "4         1.0              quick brown fox                    jumped over   \n",
       "5         1.0                    quick fox                    jumped over   \n",
       "6         1.0                          fox                    jumped over   \n",
       "7         1.0                    brown fox                    jumped over   \n",
       "8         1.0                    brown fox                    jumped over   \n",
       "9         1.0                    quick fox                    jumped over   \n",
       "10        1.0                          fox                    jumped over   \n",
       "11        1.0            Christine Lagarde                      discussed   \n",
       "12        1.0            Christine Lagarde                      discussed   \n",
       "13        1.0            Christine Lagarde                      discussed   \n",
       "14        1.0            Christine Lagarde  discussed stimulus efforts in   \n",
       "15        1.0            Christine Lagarde                      discussed   \n",
       "16        1.0            Christine Lagarde                      discussed   \n",
       "17        1.0            Christine Lagarde                      discussed   \n",
       "18        1.0  short-term stimulus efforts                          is in   \n",
       "19        1.0            Christine Lagarde                      discussed   \n",
       "20        1.0            Christine Lagarde                      discussed   \n",
       "21        1.0            Christine Lagarde                      discussed   \n",
       "22        1.0            Christine Lagarde                      discussed   \n",
       "23        1.0             recent interview                        is with   \n",
       "24        1.0                       Martin                            was   \n",
       "25        1.0      Trayvon Benjamin Martin      was African American from   \n",
       "26        1.0      Trayvon Benjamin Martin              was American from   \n",
       "27        1.0      Trayvon Benjamin Martin                            was   \n",
       "28        1.0      Trayvon Benjamin Martin                            was   \n",
       "\n",
       "                                               object  \n",
       "0                                          my pajamas  \n",
       "1                              elephant in my pajamas  \n",
       "2                                            elephant  \n",
       "3                                            lazy dog  \n",
       "4                                                 dog  \n",
       "5                                                 dog  \n",
       "6                                                 dog  \n",
       "7                                            lazy dog  \n",
       "8                                                 dog  \n",
       "9                                            lazy dog  \n",
       "10                                           lazy dog  \n",
       "11  short-term stimulus efforts in interview with ...  \n",
       "12                      stimulus efforts in interview  \n",
       "13               stimulus efforts in recent interview  \n",
       "14                                             France  \n",
       "15  short-term stimulus efforts in recent intervie...  \n",
       "16  stimulus efforts in recent interview with Wall...  \n",
       "17    short-term stimulus efforts in recent interview  \n",
       "18          recent interview with Wall Street Journal  \n",
       "19  stimulus efforts in interview with Wall Street...  \n",
       "20                                   stimulus efforts  \n",
       "21                        short-term stimulus efforts  \n",
       "22           short-term stimulus efforts in interview  \n",
       "23                                Wall Street Journal  \n",
       "24                                            African  \n",
       "25                                            Florida  \n",
       "26                                            Florida  \n",
       "27                                           American  \n",
       "28                                   African American  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "was    1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'Martin']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trayvon Benjamin Martin    4\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'Trayvon Benjamin Martin']['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 5*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform open information extraction on a modest subset of texts relevant to your final project. Analyze the relative attachment of several subjects relative to verbs and objects and visa versa. Describe how you would select among these statements to create a database of high-value statements for your project and then do it by extracting relevant statements into a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advertisement Slogans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information extraction approaches typically (as here, with Stanford's Open IE engine) ride atop the dependency parse of a sentence. They are a pre-coded example of the type analyzed in the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.2 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 15.188 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [16.7 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0105 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/bj/v89n36091c9894hp0l9r5hgm0000gn/T/tmpjuc5krst\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(smokeDF['slogan-text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>More doctors</td>\n",
       "      <td>smoke Camels than</td>\n",
       "      <td>other cigarette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>doctors</td>\n",
       "      <td>smoke Camels than</td>\n",
       "      <td>other cigarette</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   certainty       subject               verb           object\n",
       "0        1.0  More doctors  smoke Camels than  other cigarette\n",
       "1        1.0       doctors  smoke Camels than  other cigarette"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smokeStr = smokeDF['slogan-text'].to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 13.874 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [15.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0114 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/bj/v89n36091c9894hp0l9r5hgm0000gn/T/tmpfbroul4u\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(smokeStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>your throat</td>\n",
       "      <td>reacts to</td>\n",
       "      <td>delightfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Your T-Zone</td>\n",
       "      <td>will tell</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>They</td>\n",
       "      <td>'re</td>\n",
       "      <td>smooth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>They</td>\n",
       "      <td>'re smooth on</td>\n",
       "      <td>my throat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Camels</td>\n",
       "      <td>agree with</td>\n",
       "      <td>your throat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>It</td>\n",
       "      <td>has</td>\n",
       "      <td>psychological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Ivory tips</td>\n",
       "      <td>protect</td>\n",
       "      <td>lips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>flavor</td>\n",
       "      <td>protects</td>\n",
       "      <td>throat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>finest flavor</td>\n",
       "      <td>protects</td>\n",
       "      <td>throat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Luckies</td>\n",
       "      <td>are</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Luckies</td>\n",
       "      <td>are easy on</td>\n",
       "      <td>my throat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Luckies</td>\n",
       "      <td>are always kind to</td>\n",
       "      <td>your throat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Luckies</td>\n",
       "      <td>are</td>\n",
       "      <td>always kind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Luckies</td>\n",
       "      <td>are</td>\n",
       "      <td>kind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Luckies</td>\n",
       "      <td>are kind to</td>\n",
       "      <td>your throat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>go on</td>\n",
       "      <td>sweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>go</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>light</td>\n",
       "      <td>Lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>protect</td>\n",
       "      <td>my voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>throat</td>\n",
       "      <td>to</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Chesterfield</td>\n",
       "      <td>is</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Chesterfield</td>\n",
       "      <td>is best for</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>leave</td>\n",
       "      <td>clean fresh taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>leave</td>\n",
       "      <td>clean taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>leave</td>\n",
       "      <td>taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>leave</td>\n",
       "      <td>fresh taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>leave taste in</td>\n",
       "      <td>my mouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>America</td>\n",
       "      <td>in</td>\n",
       "      <td>col.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>largest selling cigarette</td>\n",
       "      <td>is in</td>\n",
       "      <td>America 's col.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Your throat</td>\n",
       "      <td>will like</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Kools</td>\n",
       "      <td>are soothing to</td>\n",
       "      <td>your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Kools</td>\n",
       "      <td>are</td>\n",
       "      <td>soothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>men</td>\n",
       "      <td>Got</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>mild men</td>\n",
       "      <td>Got</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>holiday throats</td>\n",
       "      <td>need</td>\n",
       "      <td>carton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>holiday throats</td>\n",
       "      <td>need</td>\n",
       "      <td>carton of Kools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>You</td>\n",
       "      <td>smoke</td>\n",
       "      <td>Parliaments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>You</td>\n",
       "      <td>get</td>\n",
       "      <td>lot like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>flavor</td>\n",
       "      <td>get</td>\n",
       "      <td>lot like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.404866</td>\n",
       "      <td>You</td>\n",
       "      <td>lot</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>You</td>\n",
       "      <td>'re</td>\n",
       "      <td>so smart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>You</td>\n",
       "      <td>'re</td>\n",
       "      <td>smart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>They</td>\n",
       "      <td>are</td>\n",
       "      <td>so smooth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>They</td>\n",
       "      <td>kind to</td>\n",
       "      <td>your throat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>They</td>\n",
       "      <td>are</td>\n",
       "      <td>smooth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>heart</td>\n",
       "      <td>to</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty                    subject                verb  \\\n",
       "0    1.000000                your throat           reacts to   \n",
       "1    1.000000                Your T-Zone           will tell   \n",
       "2    1.000000                       They                 're   \n",
       "3    1.000000                       They       're smooth on   \n",
       "4    1.000000                     Camels          agree with   \n",
       "5    1.000000                         It                 has   \n",
       "6    1.000000                 Ivory tips             protect   \n",
       "7    1.000000                     flavor            protects   \n",
       "8    1.000000              finest flavor            protects   \n",
       "9    1.000000                    Luckies                 are   \n",
       "10   1.000000                    Luckies         are easy on   \n",
       "11   1.000000                    Luckies  are always kind to   \n",
       "12   1.000000                    Luckies                 are   \n",
       "13   1.000000                    Luckies                 are   \n",
       "14   1.000000                    Luckies         are kind to   \n",
       "15   1.000000                          I               go on   \n",
       "16   1.000000                          I                  go   \n",
       "17   1.000000                          I               light   \n",
       "18   1.000000                          I             protect   \n",
       "19   1.000000                     throat                  to   \n",
       "20   1.000000               Chesterfield                  is   \n",
       "21   1.000000               Chesterfield         is best for   \n",
       "22   1.000000                       they               leave   \n",
       "23   1.000000                       they               leave   \n",
       "24   1.000000                       they               leave   \n",
       "25   1.000000                       they               leave   \n",
       "26   1.000000                       they      leave taste in   \n",
       "27   1.000000                    America                  in   \n",
       "28   1.000000  largest selling cigarette               is in   \n",
       "29   1.000000                Your throat           will like   \n",
       "30   1.000000                      Kools     are soothing to   \n",
       "31   1.000000                      Kools                 are   \n",
       "32   1.000000                        men                 Got   \n",
       "33   1.000000                   mild men                 Got   \n",
       "34   1.000000            holiday throats                need   \n",
       "35   1.000000            holiday throats                need   \n",
       "36   1.000000                        You               smoke   \n",
       "37   1.000000                        You                 get   \n",
       "38   1.000000                     flavor                 get   \n",
       "39   0.404866                        You                 lot   \n",
       "40   1.000000                        You                 're   \n",
       "41   1.000000                        You                 're   \n",
       "42   1.000000                       They                 are   \n",
       "43   1.000000                       They             kind to   \n",
       "44   1.000000                       They                 are   \n",
       "45   1.000000                      heart                  to   \n",
       "\n",
       "               object  \n",
       "0        delightfully  \n",
       "1                 you  \n",
       "2              smooth  \n",
       "3           my throat  \n",
       "4         your throat  \n",
       "5       psychological  \n",
       "6                lips  \n",
       "7              throat  \n",
       "8              throat  \n",
       "9                easy  \n",
       "10          my throat  \n",
       "11        your throat  \n",
       "12        always kind  \n",
       "13               kind  \n",
       "14        your throat  \n",
       "15             sweets  \n",
       "16              light  \n",
       "17              Lucky  \n",
       "18           my voice  \n",
       "19            content  \n",
       "20               best  \n",
       "21                you  \n",
       "22  clean fresh taste  \n",
       "23        clean taste  \n",
       "24              taste  \n",
       "25        fresh taste  \n",
       "26           my mouth  \n",
       "27               col.  \n",
       "28    America 's col.  \n",
       "29             change  \n",
       "30               your  \n",
       "31           soothing  \n",
       "32               cold  \n",
       "33               cold  \n",
       "34             carton  \n",
       "35    carton of Kools  \n",
       "36        Parliaments  \n",
       "37           lot like  \n",
       "38           lot like  \n",
       "39               like  \n",
       "40           so smart  \n",
       "41              smart  \n",
       "42          so smooth  \n",
       "43        your throat  \n",
       "44             smooth  \n",
       "45            content  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find at the most common subject in this story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Luckies                      6\n",
       "They                         5\n",
       "they                         5\n",
       "You                          5\n",
       "I                            4\n",
       "Kools                        2\n",
       "Chesterfield                 2\n",
       "holiday throats              2\n",
       "flavor                       2\n",
       "America                      1\n",
       "It                           1\n",
       "Your T-Zone                  1\n",
       "heart                        1\n",
       "Your throat                  1\n",
       "largest selling cigarette    1\n",
       "men                          1\n",
       "Camels                       1\n",
       "mild men                     1\n",
       "finest flavor                1\n",
       "throat                       1\n",
       "your throat                  1\n",
       "Ivory tips                   1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common subjects are **entities that refer to cigarettes** (Luckies, they, Kools, Chesterfield, Camels, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "your throat          3\n",
       "my throat            2\n",
       "you                  2\n",
       "throat               2\n",
       "easy                 1\n",
       "clean fresh taste    1\n",
       "my mouth             1\n",
       "sweets               1\n",
       "clean taste          1\n",
       "delightfully         1\n",
       "psychological        1\n",
       "content              1\n",
       "kind                 1\n",
       "Lucky                1\n",
       "America 's col.      1\n",
       "always kind          1\n",
       "fresh taste          1\n",
       "col.                 1\n",
       "best                 1\n",
       "lips                 1\n",
       "light                1\n",
       "smooth               1\n",
       "my voice             1\n",
       "taste                1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF['object'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common objects are entities that refer to or related to health issues, like \"your throat\", \"my throat\", \"my mouth\".\n",
    "\n",
    "Let's check if words about health issue are objects of cigarettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "your throat    2\n",
       "easy           1\n",
       "my throat      1\n",
       "always kind    1\n",
       "kind           1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'Luckies']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "your throat    1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'Camels']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is yes. **Cigarettes and health** are significant and often related subject-object in advertising slogans.\n",
    "\n",
    "So, let's look into what verbs connects this pair of subject and object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "are                   6\n",
       "leave                 4\n",
       "'re                   3\n",
       "Got                   2\n",
       "protect               2\n",
       "to                    2\n",
       "protects              2\n",
       "get                   2\n",
       "need                  2\n",
       "go                    1\n",
       "is best for           1\n",
       "agree with            1\n",
       "in                    1\n",
       "kind to               1\n",
       "are easy on           1\n",
       "leave taste in        1\n",
       "lot                   1\n",
       "reacts to             1\n",
       "is in                 1\n",
       "go on                 1\n",
       "will tell             1\n",
       "are kind to           1\n",
       "is                    1\n",
       "are soothing to       1\n",
       "will like             1\n",
       "'re smooth on         1\n",
       "light                 1\n",
       "has                   1\n",
       "are always kind to    1\n",
       "smoke                 1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "are                   3\n",
       "are easy on           1\n",
       "are kind to           1\n",
       "are always kind to    1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'Luckies']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kind to               1\n",
       "are kind to           1\n",
       "are always kind to    1\n",
       "agree with            1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['object'] == 'your throat']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbs (or verb phrases) between cigarettes and health are **are easy on, are kind to, agree with**.\n",
    "\n",
    "The statement I extracts from the advertising slogans is that **cigarettes (or smoking) is good to health**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anti-smoking Slogans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what I extracted from the advertising slogans, I want to figure out that \n",
    "\n",
    "1) If anti-smoking slogans also makes statements about the relationship between cigarettes (or smoking) and health\n",
    "\n",
    "2) Whether and how its statement differs from advertising slogans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antiStr = antiDF['slogan-text'].to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 15.975 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [17.8 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0102 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/bj/v89n36091c9894hp0l9r5hgm0000gn/T/tmphrrv9vwa\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "antiIeDF = stanford.openIE(antiStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>you</td>\n",
       "      <td>swallow</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.845189</td>\n",
       "      <td>tobacco</td>\n",
       "      <td>Be</td>\n",
       "      <td>Fighter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>tobacco</td>\n",
       "      <td>kills</td>\n",
       "      <td>Fighter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Cigarettes</td>\n",
       "      <td>are killers</td>\n",
       "      <td>travel in packs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Cigarettes</td>\n",
       "      <td>are killers</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Cigarettes</td>\n",
       "      <td>burn</td>\n",
       "      <td>holes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Cigarettes</td>\n",
       "      <td>burn holes in</td>\n",
       "      <td>your pocket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>your mouth</td>\n",
       "      <td>is on</td>\n",
       "      <td>fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>has</td>\n",
       "      <td>right to air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>has</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>has</td>\n",
       "      <td>right to clean air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>like</td>\n",
       "      <td>smoking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>my kids</td>\n",
       "      <td>love</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Lions</td>\n",
       "      <td>Burn</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Our planet</td>\n",
       "      <td>is</td>\n",
       "      <td>overcrowded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>puts out</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>smoking</td>\n",
       "      <td>quits</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>your</td>\n",
       "      <td>worries</td>\n",
       "      <td>lungs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Smoke worries</td>\n",
       "      <td>lungs</td>\n",
       "      <td>your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Smoke</td>\n",
       "      <td>can cause</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Smoke</td>\n",
       "      <td>can cause</td>\n",
       "      <td>slow death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Smokers</td>\n",
       "      <td>are</td>\n",
       "      <td>jokers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.789206</td>\n",
       "      <td>cigarettes</td>\n",
       "      <td>is</td>\n",
       "      <td>so yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.789206</td>\n",
       "      <td>Smoking cigarettes</td>\n",
       "      <td>is</td>\n",
       "      <td>yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.789206</td>\n",
       "      <td>cigarettes</td>\n",
       "      <td>is</td>\n",
       "      <td>yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.789206</td>\n",
       "      <td>Smoking cigarettes</td>\n",
       "      <td>is</td>\n",
       "      <td>so yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>fail</td>\n",
       "      <td>your Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>injures</td>\n",
       "      <td>your health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>someone</td>\n",
       "      <td>kill</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Smoking</td>\n",
       "      <td>scares</td>\n",
       "      <td>your Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>No-Smoking</td>\n",
       "      <td>cares</td>\n",
       "      <td>y.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>roads</td>\n",
       "      <td>lungs</td>\n",
       "      <td>your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>cigarette</td>\n",
       "      <td>does</td>\n",
       "      <td>smoking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Cigarette</td>\n",
       "      <td>is</td>\n",
       "      <td>dead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>you</td>\n",
       "      <td>are</td>\n",
       "      <td>then fool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>you</td>\n",
       "      <td>are</td>\n",
       "      <td>fool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.593731</td>\n",
       "      <td>Think</td>\n",
       "      <td>is</td>\n",
       "      <td>cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Tobacco companies</td>\n",
       "      <td>kill</td>\n",
       "      <td>their best customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Tobacco companies</td>\n",
       "      <td>kill</td>\n",
       "      <td>their customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>quitter</td>\n",
       "      <td>is</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>you</td>\n",
       "      <td>save</td>\n",
       "      <td>governm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Your money</td>\n",
       "      <td>going up in</td>\n",
       "      <td>smoke</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty             subject           verb                object\n",
       "0    1.000000                 you        swallow                    it\n",
       "1    0.845189             tobacco             Be               Fighter\n",
       "2    1.000000             tobacco          kills               Fighter\n",
       "3    1.000000          Cigarettes    are killers       travel in packs\n",
       "4    1.000000          Cigarettes    are killers                travel\n",
       "5    1.000000          Cigarettes           burn                 holes\n",
       "6    1.000000          Cigarettes  burn holes in           your pocket\n",
       "7    1.000000          your mouth          is on                  fire\n",
       "8    1.000000            Everyone            has          right to air\n",
       "9    1.000000            Everyone            has                 right\n",
       "10   1.000000            Everyone            has    right to clean air\n",
       "11   1.000000                   I           like               smoking\n",
       "12   1.000000             my kids           love                    me\n",
       "13   1.000000               Lions           Burn                    it\n",
       "14   1.000000          Our planet             is           overcrowded\n",
       "15   1.000000                  it       puts out                   you\n",
       "16   1.000000             smoking          quits                   you\n",
       "17   1.000000                your        worries                 lungs\n",
       "18   1.000000       Smoke worries          lungs                  your\n",
       "19   1.000000               Smoke      can cause                 death\n",
       "20   1.000000               Smoke      can cause            slow death\n",
       "21   1.000000             Smokers            are                jokers\n",
       "22   0.789206          cigarettes             is          so yesterday\n",
       "23   0.789206  Smoking cigarettes             is             yesterday\n",
       "24   0.789206          cigarettes             is             yesterday\n",
       "25   0.789206  Smoking cigarettes             is          so yesterday\n",
       "26   1.000000             Smoking           fail           your Health\n",
       "27   1.000000             Smoking        injures           your health\n",
       "28   1.000000             someone           kill                   you\n",
       "29   1.000000             Smoking         scares           your Health\n",
       "30   1.000000          No-Smoking          cares                    y.\n",
       "31   1.000000               roads          lungs                  your\n",
       "32   1.000000           cigarette           does               smoking\n",
       "33   1.000000           Cigarette             is                  dead\n",
       "34   1.000000                 you            are             then fool\n",
       "35   1.000000                 you            are                  fool\n",
       "36   0.593731               Think             is                  cool\n",
       "37   1.000000   Tobacco companies           kill  their best customers\n",
       "38   1.000000   Tobacco companies           kill       their customers\n",
       "39   1.000000             quitter             is                   bad\n",
       "40   1.000000                 you           save               governm\n",
       "41   1.000000          Your money    going up in                 smoke"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antiIeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cigarettes            4\n",
       "you                   4\n",
       "Smoking               3\n",
       "Everyone              3\n",
       "Tobacco companies     2\n",
       "tobacco               2\n",
       "cigarettes            2\n",
       "Smoke                 2\n",
       "Smoking cigarettes    2\n",
       "your                  1\n",
       "quitter               1\n",
       "Smokers               1\n",
       "Smoke worries         1\n",
       "cigarette             1\n",
       "Lions                 1\n",
       "Cigarette             1\n",
       "I                     1\n",
       "Think                 1\n",
       "No-Smoking            1\n",
       "someone               1\n",
       "Your money            1\n",
       "smoking               1\n",
       "my kids               1\n",
       "your mouth            1\n",
       "Our planet            1\n",
       "roads                 1\n",
       "it                    1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antiIeDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the most common subjects in anti-smoking slogans are also entities related to cigarettes or smoking (cigarettes, smoking, smoke, tobacco, smoker)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "you                     3\n",
       "it                      2\n",
       "so yesterday            2\n",
       "yesterday               2\n",
       "your                    2\n",
       "Fighter                 2\n",
       "your Health             2\n",
       "smoking                 2\n",
       "slow death              1\n",
       "holes                   1\n",
       "your health             1\n",
       "travel in packs         1\n",
       "travel                  1\n",
       "their customers         1\n",
       "right                   1\n",
       "right to air            1\n",
       "smoke                   1\n",
       "fire                    1\n",
       "lungs                   1\n",
       "jokers                  1\n",
       "governm                 1\n",
       "then fool               1\n",
       "me                      1\n",
       "y.                      1\n",
       "their best customers    1\n",
       "your pocket             1\n",
       "cool                    1\n",
       "overcrowded             1\n",
       "dead                    1\n",
       "fool                    1\n",
       "right to clean air      1\n",
       "death                   1\n",
       "bad                     1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antiIeDF['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "your Health    2\n",
       "your health    1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antiIeDF[antiIeDF['subject'] == 'Smoking']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subject of smoking is also related to the object of health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fail      1\n",
       "scares    1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antiIeDF[antiIeDF['object'] == 'your Health']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "injures    1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antiIeDF[antiIeDF['object'] == 'your health']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "injures    1\n",
       "scares     1\n",
       "fail       1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antiIeDF[antiIeDF['subject'] == 'Smoking']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "are killers      2\n",
       "burn             1\n",
       "burn holes in    1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antiIeDF[antiIeDF['subject'] == 'Cigarettes']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the verbs that connect subject \"smoking\" and object \"health\" in anti-smoking slogans are totally opposite to advertising slogans, such as **fail**, **scares**, **injures** and **are killers**.\n",
    "\n",
    "Therefore, the statement I extract from anti-smoking slogans are **smoking (or cigarettes) are bad to health**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Statements into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, based on the statement that I extracted from cigarettes advertising slogans and anti-smoking slogans, the stories behinds these two kinds of slogans are different. Suprisingly, they both trying to construct a statement about the relationship between smoking and health in their narratives. However, while the statement of advertising slogans is **Smoking is good to health**, the statement of anti-smoking slogans is **Smoking is bad to health**.\n",
    "\n",
    "By claiming that smoking is good to health, what the cigarettes company trying to do is to legitimizing smoking, which can finally contribute to the cigarettes marketing. However, anti-smoking slogans aim to de-legitimizing smoking by emphasizing on smoking's bad effect on health.\n",
    "\n",
    "Therefore, we can see that the important thing here is not whether smoking is good or bad for our health. What matters here is how people interpret cigarettes and smoking and how they make other people believes their statements. This feature of slogans can be also generalized to lots of issues like political campaign and propaganda and social movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
